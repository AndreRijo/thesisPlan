%!TEX root = ../template.tex
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% chapter3.tex
%% NOVA thesis document file
%%
%% Chapter with a short latex tutorial and examples
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\typeout{NT FILE chapter3.tex}%

\chapter{Research Statement}
\label{cha:research_statement}

%Notas reuniao: Usar o plano para a bolsa da FCT, re-fazendo o que ja foi feito/o que vai ser feito
%Ainda sera preciso implementar outro mecanismo de control de versoes, mas o mais dificil ja esta

\todo{Is this okay, or is this more like Introduction stuff?}

In this work we propose to address the issue of serving large-scale analytic queries efficiently in a geo-distributed scenario.
As discussed previously in Section \ref{subsec:queries}, some systems attempt to address this.
However, each of those systems presents some of the following shortcomings: costly view maintenance, limited amounts of data (e.g. data of last week), very stale data, costly queries, high storage costs, all data of a view having to be present in one server, etc.

In this work we aim to propose algorithms and techniques that address the mentioned shortcomings, allowing for a system to provide analytical queries efficiently, alongside ``normal'' queries of an application.
This allows for the system to serve the needs of an application while allowing business analysts to get statistics of the system in real time, without affecting the functioning of the application.

We envision a situation in which large-scale web applications deploy many datacenters scattered around the world (geo-distributed), possibly in the cloud, in which analytical queries are expected to be executed quickly without stopping the system, enabling relevant business or data-load decisions.
It is thus necessary to study how can a system scale in this scenario, ensuring low latency for clients, high throughput, good fault tolerance and useful consistency guarantees, all while keeping system costs reasonable.
We propose a solution to this with our algorithms and a database implementation, PotionDB.
PotionDB is a distributed, geo-replicated database being developed in the context of this work, implementing the techniques and algorithms that we design.
Thus, PotionDB will be used to both demonstrate and evaluate our concepts.
At the end, PotionDB should demonstrate our vision, providing a scalable database for a geo-replicated service which serves both application and analytical queries efficiently, while also handling updates and ensuring low latency.

The next subsections are organized as follows.
First, Section \ref{sec:choices_and_motivation} motivates our design choices and algorithms.
Then, Sections \ref{sec:replication_model}, \ref{sec:views_research_statement} and \ref{sec:version_management} each present one of our core contributions, the problems they try to solve and how we solve them.
Section \ref{sec:potiondb_structure} presents PotionDB's architecture and details on our transactional algorithm.
Section \ref{sec:potiondb_evaluation} discusses how we evaluate PotionDB and plan to keep evaluating.
Finally, Section \ref{sec:contributions} summarizes the current contributions, while Section \ref{sec:next_steps} discusses the next directions for this work, current limitations and upcoming contributions.

\todo{Do I need to talk about how the algorithm for committing transactions works in detail? I lightly talk about it in Section \ref{sec:potiondb_structure}}

\todo{Where should I mention about the support for querying/updating objects that are replicated in different servers (e.g., client contacts server A, objects are in server B). I feel like that is important to mention, but it is not essential to any of PotionDB's algorithms - we provide it for completeness mostly.}

I don't know yet the organization of this but... I want to address the following topics:
\begin{itemize}
	\item reducing storage costs - partial + uniform replication
	\item providing useful guarantees for applications without strong consistency - causality, PSI, CRDTs, read-write transactions, cross-server transactions
	\item how the replication works with all of the above
	\item the structure of PotionDB (including internal partitioning for multi-core efficiency)
	\item the version management/reads in the past? Likely under PSI part
	\item Views.
\end{itemize}

And for current limitations:
\begin{itemize}
	\item Storage/garbage - need to do garbage collecting of old operations/effects
	\item Dealing with high update rates more efficiently - under snapshot isolation, as of now, throughput decreases heavily when there is many updates coupled with many reads (e.g., 50-50). Under read committed, the loss on throughput is much more reasonable.
	\item More consistency levels - to better meet the needs of the application; how different consistency levels coupe with our replication scheme.
	\item Automatic view updating - as of now, it is the client that updates the views.
	\item Invariants
	\item dynamic partitioning
	\item new replicas/replicas removal (already supported but on a light level from what I recall)
	%\item datacenter consideration (e.g., sharding inside a datacenter)? Do we consider this, or no? (I am inclined to say no)
\end{itemize}

So, after meeting...
Talk about what is already done. Talk about what is left to do according to the plan in the PhD grants.

\section{Design choices and motivation}
\label{sec:choices_and_motivation}

In a geo-distributed scenario, latency plays a major role due to the distance between datacenters.
Strongly consistent solutions need either to coordinate between faraway datacenters or have data mastered in a region, both leading to high latency on most data accesses.
In many scenarios, the added latency from these solutions is unacceptable as it has consequences on user satisfaction and thus on product viability \cite{???}. %TODO: Cite references that claim higher latency = client loss. One such is Amazon iirc.
Network partitions may also affect availability of the service, further exacerbating the problem.

Thus, many systems provide weak forms of consistency in order to provide low latency for their clients.
However, weak consistency has many problems as discussed previously (Section \ref{sec:consistency}), namely making application development more difficult due to exposure of concurrency animalities and limited data objects.
We tackle those in our work by considering causal consistency \cite{???} and by providing in our prototype CRDTs \cite{???} (Section \ref{sec:consistency}).
As previously discussed, causal consistency avoids certain anomalies, thus reducing burden on the application programmer.
CRDTs further reduce the burden by solving concurrency conflicts automatically.
We provide many kinds of data types implemented as CRDTs, thus ensuring many applications can find data types appropriate for their needs.
Some existing solutions, e.g., MemCached, Dynamo and COPS \cite{???} only provide registers, which can make the usage of simple, common data abstractions such as sets, challenging.
In PotionDB we provide CRDTs for maps, sets, counters, registers, as well as for statistical data such as averages, max/min and TopK.
It is noteworthy that our maps support embedded CRDTs as values, which is challenging, while TopK supports removals.
Furthermore, we also provide a TopK variant that supports increments and decrements to each entry, while keeping data replicated short.
The original design for TopK with increments did not support decrements, which is not trivial given the way data is replicated.

In geo-distributed scenarios, services tend to have a considerable amount of datacenters, in order to have a datacenter close to the client independently of his location.
The higher the number of datacenters, the higher the replication, networking and storage costs, as data is replicated everywhere.
This is even more noticeable when services are deployed on the cloud, as storage, networking and processing power is often charged.
Fortunately, not all data is relevant everywhere - e.g., the data related to a Portuguese costumer is unlikely to be relevant in Asia.
Partial replication is thus attractive, as it can lower all the aforementioned costs - less data is sent to each datacenter, thus less operations need to be processed and less data is stored in each place.
However, partial replication is not a panacea - if clients try to contact a datacenter and the data needed is not there, another more faraway datacenter will have to be contacted.
In such situation, having partial replication is worse than replicating fully.

A challenging scenario, which we address in this work, is how can materialized views be provided under partial replication, without making restriction on which data views can be built upon.
That is, we will provide views which can span data that is not all replicated in a single replica.
To the best of our knowledge, this is a novel problem when considering weak consistency and geo-distribution.
It is also a very relevant matter as it has the potential to provide very fast access to statistics concerning large amounts of data, with a simple query in any datacenter, without needing coordination for each query on the view, or some sort of view composition algorithm.

Another concern with views is that it is believed not every query can have views, as storage and maintenance costs increase considerably as more views are added \cite{???}.
We counter this in three different ways.
First, we make usage of non-uniform replication \cite{???}, which reduces the amount of data stored in each server for a view.
E.g., to maintain a top 100 with 10000 entries, not all entries need to be replicated everywhere.
Second, we provide partial replication support for views, thus if there is locality to their data it is possible to replicate the views only in relevant servers.
Third, our views are incrementally maintained - one update to an object leads to one update in each view for which the object is relevant.

It is noteworthy the combination of partial replication with non-uniform replication, as well as the support for views having data not locally replicated is novel.
This required us to develop a new replication algorithm, as well as one to execute transactions that ensures that changes to views and the objects referred by them are visible atomically.
Furthermore, our algorithms must ensure views are kept up-to-date, even for entries correspondent to objects not locally replicated.

%The three big topics to talk about:
%- Replication model
%- Views
%- Testing/TPC-H

\section{Replication model}
\label{sec:replication_model}

One key aspect of our work is the definition of a new replication model that is well suited for geo-replication, taking leverage and combining existing techniques for effectiveness.
More precisely, we define the following goals for our replication model:
\begin{itemize}
	\item Appropriate for long distance replication - this implies avoiding blocking mechanisms or having to coordinate with faraway replicas to make progress;
	\item Data efficient - in general, reduce the amount of data sent to each server.
	In concrete, it must support partial replication, in order to support each server replicating different subsets of data.
	\item Consistent - in our case, keeping consistency is challenging and must be supported by the replication algorithm.
	Views can depend on data that is split across many servers, without a single server having all the data.
	However, views must stay correct and up-to-date.
	Thus, it is necessary to ensure that when an item is updated, views are also updated in every replica of the views even if said item is not replicated there, without requiring the item's update or data to be sent to said servers.
\end{itemize}

Our replication algorithm is asynchronous.
Thus, servers can continue executing operations while replication happens in the background, without affecting correctness, even if latency between servers is high.
Our algorithm ensures every server will receive the relevant updates and, for each object, all replicas of said object will eventually have the same state.
For data efficiency, our replication algorithm supports two mechanisms - partial replication and non-uniform replication (see Sections \ref{subsec:partial} and \ref{subsec:nonuniform} respectively).
Partial replication permits to have each server replicate a different subset of data.
This allows to make use of data locality as not all data is relevant everywhere, e.g., in an e-commerce application, data of a Portuguese customer is unlikely to be relevant in Asia.

We make usage of partial replication by only replicating, to each server, operations for objects the target server replicates.
We do so as follows.
First, we assume objects are arranged into ``groups'', which we call \emph{buckets}.
Each object can be assigned to one bucket, and each server can replicate one or more buckets.
Each server must register which buckets they are interested in (e.g., by using a publish-subscribe service such as RabbitMQ \cite{???}).
When replicating a transaction, the transaction is split into parts - one for each bucket present in the transaction.
Each part is sent to all replicas of the part's bucket.
After all parts are sent, a message is sent to all involved servers signalling that the transaction is complete.

\todo{There is actually an optimization possible, related to sending the special message signalling the end of the transaction - when we send many transactions in a group, we only need to send this special message on the last one. Should this be mentioned?}

Non-uniform replication allows to further reduce storage and replication costs, by reducing the amount of operations that need to be replicated for a given object.
E.g., for an object representing a top 100, an update to an object in position 10000 is unlikely to be relevant.
However, non-uniform replication also implies providing support for ``special operations'' that are generated when an operation is applied.
E.g., a remove from an element in the top 100 may force some update that was previously not relevant to be replicated, as a new element may be joining the top. 
\todo{I am not sure if the explanation of the implications of supporting non-uniform replication is understandable/clear enough. Specially not sure on the last part "as a new element may be joining the top." - from what I recall, said element would had already been replicated?}

Supporting non-uniform replication in our design has two implications.
In terms of the replication algorithm, whenever a transaction with non-uniform objects is executed, the special operations are propagated to relevant replicas grouped in one transaction.
It also has implications on the transactional algorithm, which we detail later in Section \ref{sec:potiondb_structure}.

Finally, our replication algorithm must maintain consistency.
We aim to provide causal consistency across servers.
Thus, our replication algorithm ensures operations are sent according to causal order, and that operations are also applied according to causality.
In an implementation, this can be ensured by using, e.g., vector clocks \cite{???}.
The algorithm that applies transactions ensures changes for all buckets are visible at the same time, e.g., by making usage of the mentioned vector clocks to control which updates are visible or not.

\todo{Should I mention contributions here? Or group at the end?}

%Mention the goals of our replication model

%End with saying what are the contributions?

\section{Views}
\label{sec:views_research_statement}

Another key aspect of our work is the support of materialized views in a weak-consistency, geo-distributed scenario, which is novel to the best of our knowledge.
We identify the main following problems in supporting materialized views:
\begin{itemize}
	\item Views must be kept up-to-date efficiently - an update to an object should not imply a total rebuild of a view;
	\item Changes to objects and their related views must be atomic - it should not be possible, in the same transaction, for a client to observe a state in which the object was already updated but one or more of its views were not. Views must stay consistent with the objects they refer to;
	\item Views must be updated even for entries whose objects are not locally replicated, without breaking causality.
	\item Access to views must be efficient - queries to views should be in same order of complexity as queries to other objects.
	Thus, the required data to reply queries of views should be readily available without any complex or time-consuming calculations.
	\item Storage and update cost of views should be minimized when possible - as was seen in Section \ref{sec:data_access}, both have been a problem with adopting materialized views in commercial databases and is an hot topic of research.
	\end{itemize}
\todo{Is the last point of the list repetitive with the first one?}
%Mention they are objects like any other, but are updated automatically

We tackle the mentioned problems as follows.
To keep views up-to-date efficiently, we rely on incremental view maintenance - more precisely, an update to an object generates one update for each relevant view, which changes just the necessary data in the view without having to recalculate it from the ground up.
In our prototype of PotionDB, views are implemented as CRDTs like the remaining objects - thus updating a view corresponds to a CRDT update.
Since views are CRDTs, queries are also quick - the necessary data to access the query is already present and should be supported by employing an appropriate CRDT.
For example, for a top-k query, a top-k CRDT is used. For a query that requires multiple sums, with one sum per entry, a map CRDT with embedded counter CRDTs can be used.
As was hinted in the previous section, non-uniform replication helps reduce the storage and replication cost of views.
Partial replication also helps, both by not requiring that all objects related to views be replicated alongside a view \footnote{We do require that, for each object that may be relevant for a view, at least one server replicating said object also replicates the view. We believe, in practice, this is not a limitation - if a view concerns global data, it is likely relevant everywhere and, thus, would be replicated everywhere nonetheless.}, as well as allowing for views to only be replicated in desired servers, if so is intended.
This is useful as despite many views being global, e.g., top sales of a product, some may also be local, e.g., top sales of a product in Portugal. The former does not subdue the latter, as the latter may be used, e.g., to adjust supplies or promotions of certain products in Portugal only.
The latter view does not need to be replicated everywhere, thus saving storage costs.

\todo{Not sure if the footnote above should be in the paragraph below, or if it is okay in that paragraph. Also, is it okay as a footnote, or should be included in the main text?}

Consistency between views and their objects is maintained by ensuring views are always updated in the same transaction as their object(s).
As previously mentioned, each transaction is made visible atomically - thus, if views and their objects are updated in the same transaction, then from the client point of view, changes to views and objects are observed atomically.
Causal consistency also prevents some concurrency anomalies from being observed, as discussed in Section \ref{subsec:weak}.
Views are kept up-to-date even if their related objects are not replicated in the target server due to how the replication algorithm previously explained works - the algorithm will detect that the view update corresponds to a bucket replicated by the target server and send that part of the transaction (and all other parts correspondent to buckets the target server replicates).
The target server will then apply the view updates, respecting causality.
Thus, views are kept up-to-date correctly.

\section{Version Management}
\label{sec:version_management}

In order to ensure each transaction can see a consistent snapshot of the database, while still allowing multiple transactions to occur in parallel, it is necessary to provide specific versions of any given object.
More precisely, it is necessary to provide the version required by any ongoing transaction.
Thus, some mechanism to provide old versions is needed.

Some solutions, e.g., Cure, keeps old versions around, garbage collecting them when no longer needed \cite{cure}.
In this work, we decided to explore a different approach - instead, we keep only the latest version (and optionally, we may cache a few other versions for performance when appropriate, but not as a requirement).
Alongside the latest version, we keep two logs for each object - the list of updates executed, and the list of ``effects'' of each update.
We define an effect as the changes an operation had on an object when it was executed.
Both logs are needed, as the same operation may imply different changes to the object depending on when it was executed.
E.g., a remove from a set may or not remove an object from the set, depending on the existence of concurrent adds for the same element.

Our algorithm thus works as follows.
First, given a target version and the current version, we ``undo'' the effects, until we are at a version for which the vector clock is smaller or equal in every entry than the target version.
Afterwards, we re-apply the operations needed to reach the target vector clock.
The reason for the need to re-apply operations is due to concurrent operations.
Operations that are concurrent can be re-applied in any order, but causality must still be maintained between two non-concurrent operations (e.g., operation A may be concurrent to operation B and C, but B and C are causally related - A can be re-applied at any time, but B and C must be applied in order).

\todo{Is this explanation of the version thing okay? Also, should I include some scheme/image with an example?}

\section{PotionDB structure}
\label{sec:potiondb_structure}

\todo{I will include some picture showing here the structure of PotionDB. I am thinking of some simple scheme showing Front End + Materializer + TransactionManager + Log + Replication, and servers interacting (as well as clients)}

The main goal of our database prototype, PotionDB, is to demonstrate how can our algorithms and techniques be implemented, as well as evaluate their performance and practically of usage.
As we test their performance, lessons can be learned which can then be used to improve our algorithms.
It also serves as a way to compare with other existing solutions in the literature.

To represent our objects and views, we implement many kinds of CRDTs.
This allows us to provide objects and views appropriate for many kinds of applications and queries.
PotionDB's architecture takes inspiration from Cure \cite{???} and AntidoteDB \cite{???}.
Our clients interact with PotionDB using Google's protobufs \cite{???}.
Our protobuf interface extends AntidoteDB's - thus, AntidoteDB's clients are compatible with PotionDB.
We extend the interface with support for new CRDTs and some extra operations - namely, we provide support for partial reads on CRDTs.
This allows clients to query only part of the CRDTs state, e.g., in a set to check if an element is on the set; on a top 100, to obtain only the top elements above a certain value, etc.

Internally, we use a similar separation of components to Cure - we keep the Transaction Manager, Materializer, Log and Replication components:
\begin{itemize}
	\item Materializer (Mat) - responsible for storing the objects of PotionDB. Internally we partition the materializer in a pre-defined number of partitions.
	This allows many transactions to execute in parallel, without the need for usage of locks, thus leading to a good usage of multi-core CPU which are common nowadays.
	Each partition of the Materializer is also responsible for executing reads and updates of each object it stores, as well as compute old versions of objects when needed to serve a transaction.
	\item Transaction Manager (TM) - responsible for receiving client requests, after being processed by PotionDB's frontend, and execute said requests.
	The Transaction Manager splits each transaction in parts correspondent to the Materializer's partitions that are accessed by the transaction.
	Read-only transactions can execute in parallel with no coordination.
	Our transactional algorithm ensures that transactions with writes only coordinate among the partitions involved - thus other transactions can still execute in parallel if the partitions involved are disjoint.
	Transaction Manager also ensures all reads and updates of a transaction are executed upon the same version.
	To keep track of causality and versions, we make usage of vector clocks, more precisely, ClockSI \cite{???}.
	\item Log - keeps an in-memory log of the updates. This component is mainly used to feed the Replication component. It would be possible, if desired, to implement a disk-based Log for durability purposes.
	\item Replication - responsible for implementing the replication algorithm, sending and receiving transactions to/from other servers.
	It groups transactions for efficiency, but also separates each transaction in buckets, so that each server only receives the necessary data.
	When receiving, it is also responsible for re-grouping the buckets of a transaction and passing the entire transaction to the Transaction Manager.
	We leverage on RabbitMQ \cite{???} to implement the communication between servers, making usage of its publish-subscribe nature to ensure each server only receives updates for the buckets it replicates.
\end{itemize}

Internally, our Transaction Manager uses a local two-phase commit protocol (2PC, check Section \ref{subsec:existingSystems}) to coordinate the partitions on a read-write transaction.
The protocol executes very quickly as it only involves communication between threads, i.e., no network communication.
An important point is the changes required to our transactional algorithm to support non-uniform replication.
When a transaction with non-uniform objects is executed and generates new operations, those operations are grouped, from the partitions involved.
Afterwards, a new commit is coordinated with the relevant partitions using the current clock, and is then sent to the Replication component to be replicated to other servers.

\section{Evaluation of PotionDB}
\label{sec:potiondb_evaluation}

To evaluate our prototype and the implemented algorithms, specific client programs and other tools have been developed.
Namely, we highlight the implementation of the TPC-H benchmark \cite{???}.

TPC-H is a benchmark designed for strongly consistent, relational databases.
It represents a large-scale e-commerce application which manages both the sales of products, lists of customers, stocks, suppliers and so forth.
The smallest size of the dataset defined by TPC-H has millions of entries in its tables.
This benchmark is aimed to evaluate how efficiently can a database reply to complex analytic queries, with those queries answering important business questions.
Many of those queries require data from millions of entries to be potentially processed.
For example, without a view or an efficient plan, query 3 may require scanning the entire lineitems table, which has millions of entries, as well as do joins. 

Implementing the TPC-H benchmark implied cumbersome and non-straightforward work.
First, to define how the tables (relational) would be stored in PotionDB (key-value store).
Secondly, to convert the data definition from SQL to protobufs supported by PotionDB.
Third, analyse which queries to support, as well as how to represent them with existing datatypes, adding new ones if needed.
This implied defining materialized views with the existent datatypes, considering how to efficiently answer each query and keep it up-to-date.
Fourth, write all the code related with executing the queries, defining objects, updating objects and views, as well as execute the benchmark itself.
Further work was done in order to test multiple configurations of PotionDB for comparison - e.g., effects of grouping updates versus splitting; PotionDB's solution for views versus another solution which only supports views of data locally replicated (which implied extra code work to update  and query this variant!), to list just a few.

The implementation of the benchmark is useful to prove the viability of a weakly consistent database supporting complex analytic queries that, without materialized views or an ad-hoc solution, would require accessing millions of objects in a typical key-value store.
It also allowed to access the effectiveness of our solution - indeed, on our experiments, we were able to answer TPC-H queries at rates of up to few millions of operations per second.
\todo{Should I remove the part about the results? Mention the Grid project? Or mention about the setup/include some results? Also this is under ``read commited'' consistency, albeit snapshot isolation should have similar throughput if update rate is low.}

Implementing TPC-H is also one way to demonstrate the expressiveness of PotionDB's CRDTs - as of this date, six complex TPC-H queries where implemented in PotionDB using CRDTs.
This includes support for many SQL operators, such as group by, limit by, order by, where restrictions with inner queries, etc.
We have also already analysed that all other TPC-H queries would be possible to support efficiently with PotionDB's existing CRDTs.

We have also defined other specific tests, e.g., to access the correctness of the implementation of the replication algorithm, as well as evaluate the performance of each CRDT.
Much work has been done in evaluating PotionDB in order to support our effort to prepare and submit a paper to VLDB.
As of the time of writing of this document, said paper is a work in progress.
We believe the current methods and code to evaluate PotionDB will prove useful to also evaluate also our future work.

\section{Contributions}
\label{sec:contributions}

In this section we now summarize the already achieved contributions with this work:

\begin{itemize}
	\item A new replication algorithm, combining partial, non-uniform and geo replication;
	\item A transactional algorithm with support for snapshot isolation at the server level, while coping with causal consistency across servers and with support for both partial and non-uniform replication;
	\item An algorithm to query and update objects not locally replicated;
	\item Definition of new CRDTs (e.g., max/min, average CRDTs), as well as extensions to existing CRDTs (e.g., TopK, TopSum);
	\item Support for materialized views under causal consistency, where updates to objects and their views are visible atomically;
	\item Support for materialized views to refer to data not locally replicated + non-uniform and partial replication of views to reduce their storage and replication costs;
	\item A new form of incrementally maintaining views, with simple CRDT update operations;
	\item An algorithm to reconstruct the state of any object at any given point in time, by using the actual object state, a list of operations and a list of effects;
	\item An implementation of the TPC-H's dataset, update operations and a subset of the queries, as well as clients to implement the TPC-H benchmark;
	\item A demonstration on how a key-value store with a rich interface and objects (namely CRDTs) can provide answers even for complex analytic queries that are usually addressed by relational databases. Also shows how SQL queries can be converted to CRDT objects.
\end{itemize}

\todo{Important note: The third contribution is not detailed anywhere in this document!!!}

\todo{Any suggestions for more contributions?}

We note that, to the best of our knowledge, our replication algorithm is novel, as the combination of the mentioned replication features is novel.
Materialized views in a geo-distributed, weakly consistent, with partial and non-uniform replication support, without requiring all relevant objects to be present in the same server, is also novel, as is the incremental updating of views with CRDTs' update operations.

\section{Next steps}
\label{sec:next_steps}

Given the current algorithms done so far, we identify the next steps in our work as follows.

\subsection{Version Management}

Our current solution for managing versions suffers from one considerable limitation - it does not support garbage collection.
Supporting efficient garbage collection is not straightforward - it is necessary to determine how old the information must be in order to be safe to be garbage collected.
If information too recent is deleted, ongoing transactions may have to be aborted (as they need garbage collected versions) and thus affect performance and user experience negatively.

One way to address this is to, periodically, delete old information (old updates and effects) that are guaranteed to no longer be necessary.
In PotionDB, this can be achieved by checking which partition has the oldest clock (or by keeping track of a clock that is known to be ``safe'' in all partitions), and deleting logs of updates and effects older than said clock.
It will be necessary to consider ongoing transactions, possibly by delaying the deletion of old data, using an even older clock or restarting the transaction with a more recent clock.
Finally, the cleaning should be done when system's load is low, or in the background to avoid affecting performance.

Another related task is to implement other solution(s), namely a solution where instead of re-calculating versions on demand, a list of all versions (up to a certain point in time) are kept in memory.
The idea is to compare our solution against this solution(s) to evaluate the benefits of ours.

In conclusion, this task includes the definition of a garbage collecting algorithm for version management, namely obtaining a clock that is safe to clean and cleaning the old versions without stopping the system.
Further included is implementing said algorithm plus another solution of version management, comparing both and assessing the benefits of our solution.

\subsection{Consistency levels and invariants}

Currently, our system is able to provide causal consistency or read committed consistency.
While this is already useful, as we provide both a solution for when useful consistency guarantees are necessary (causal) and for when high performance is key (read committed), it is still limited.
Other consistency solutions, such as red-blue, read-your-writes + monotonic-reads, or some form of strong consistency (e.g., snapshot isolation) would be interesting to support.
Ideally, we aim to provide support to running transactions at a given consistency level alongside others running at different consistency levels, e.g., one transaction running as read committed while another runs at causal.
One way to ensure so is to make the reads responsible for ensuring the desired consistency level.
I.e., transactions with writes would still be atomic to ensure objects (and their views) stay consistent, while reads executing under lower consistency guarantees can, e.g., read objects at different versions without internal coordination to improve performance.

Another important feature to support, which is usually desired by application developers, would be invariants.
An invariant ensures that some user-defined condition on a given object(s) is always true at any point in time.
This is useful to, e.g., ensure an item's stock never goes negative.
Supporting this under weak consistency is challenging as servers execute operations concurrently without synchronization.
Some solutions already exist in the literature, e.g., \cite{boundedCounter}, which suggests for the case of numeric invariants in counters, to distribute among replicas how much they can decrement/increment safely without breaking the invariant.
A similar principle could potentially be used for other kinds of objects, as well as to ensure view's invariants are not broken when related objects are updated.

We envision that supporting invariants in PotionDB would be specially useful given our provision of materialized views.
This would allow programmers to define invariants over state that represents/summarizes multiple objects, easing application development.
Supporting invariants on materialized views in a weakly consistent, geo-distributed system would be novel.
It is also challenging, as invariants on views also imply restrictions that must be kept on referred objects.

\todo{Do we really want to provide invariants on views? Sounds quite challenging.}

\subsection{Dynamic partitioning}

Our work already provides support for partitioning - in fact, partial replication is one of the focus of this work.
However, as it was seen in Section \ref{subsec:partial}, in some applications access patterns may change as time goes by, or even data locality itself (e.g., an user which travels to another region).
Some of the systems analysed in Section \ref{subsec:existingSystems} include support for dynamic partitioning.

In our work we aim for applications which can be served with mostly static partitioning, i.e., where data has geographical locality on its access patterns.
However, to accommodate some changes in access patterns, as well as e.g. support re-partitioning when new replicas join the system, we aim to provide dynamic repartitioning.

PotionDB already has support for addition of new nodes.
Newly added nodes can replicate any existing or new buckets.
Our current protocol ensures the new nodes will be informed of previous operations for buckets they replicate.
However, it is still needed to extend this protocol and design an algorithm which ensures the following:

\begin{itemize}
	\item Support for nodes leaving the system (e.g. due to a crash) and their recovery;
	\item Efficient data transfers (transfer only relevant objects, compact operation history, etc.);
	\item Dynamic partitioning - add or remove buckets from existing servers.
\end{itemize}

The last goal is particularly challenging, as the repartitioning may happen while multiple transactions are ongoing, potentially for the buckets being added/removed.
We must ensure no update is lost during repartitioning and that queries can still be served.
Consistency between views and objects must also be maintained even if views or objects referred by views are being re-partitioned.

\subsection{Improvements on view maintenance}

\todo{I don't know if this section should be present, as I do not know if this has much research relevance + may give a lot of work depending on the solution.}

Our algorithms for view maintenance are quite efficient, as some preliminary results already show.
However, the usability of view maintenance is rather limited in our prototype.
As of now, clients themselves have to bundle view updates with object updates.

Ideally, it would be desirable for view updates to be fully automatic - the application developer would specify views and which objects to build the views from using some form of language (e.g., SQL).
Then, view updates would automatically and efficiently be generated without the developer having to write how to update the view.
By itself, finding a solution for this in the context of object-based databases, with weak consistency, would be an interesting research problem.
However, we consider this problem to be outside of the scope of our work.

A different, more realistic approach, would be to support the application developer to define how a view update should be generated, given an update to an object.
This would require a mechanism to specify objects that should affect a given view.
It also requires some way to specify the view update.

A solution for this is by using triggers in PotionDB, as some relational databases do \cite{???}.
A trigger is a piece of code which executes as a consequence of an action which happened in a given object.
For instance, for a given view, a trigger could be defined that applies on all objects matching a certain criteria, whenever one of those objects is updated.
The trigger would then generate an update for the view in the same transaction.
Our prototype already has initial support for triggers, however the specification of view updates is not expressive enough for some views.
This can be fixed by, e.g., allowing the definition of custom code for triggers, or using some specific data manipulation language, e.g., \cite{???}.

\section{Expected contributions}
\label{sec:expected_contributions}

In this section we now summarize the expected upcoming contributions of this work:

\begin{itemize}
	\item A garbage collection algorithm for systems implementing SI with vector clocks. This algorithm will be able to determine a safe clock in all partitions and do the cleaning without affecting considerably the system's performance;
	\item A performance comparison between our state reconstruction algorithm and traditional solutions for keeping old versions of objects;
	\item a weakly consistent, geo distributed database supporting multiple consistency levels with view support;
	\item invariants support in a weakly consistent environment. Support for invariants on objects (views) whose state depends on other objects;
	\item an algorithm for dynamic partitioning compatible with snapshot isolation (server-level) and causal consistency (cross-servers), without loss of updates or stopping serving queries. This re-partitioning will work both for creating/removing partitions, adding/removing servers, and properly support views and their related objects;
	\item A system where application developers can define views, how to update them and not worry about having to keep views manually up-to-date or high maintenance costs;
\end{itemize}

It is noteworthy that our contributions will always have to ensure the consistency between views and their objects is maintained.
This makes our contributions both more challenging and novel.

\todo{Any more future contributions (i.e., contributions from work yet to be done)?}
%Stuff to do later?
%Garbage collection - figuring out versions that will no longer be needed
%If said detection fails, can re-issue a transaction
%Consistency levels?
%Already support snapshot isolation & monotonic reads (not really read your writes at the moment, but I should improve this). It's also read commited.
%Data is still updated according to snapshot isolation, just that reads do not see it that way when working on fast mode.
%Strong consistency?
%Automatic views/view updates?
%As of now, the client keeps the views up-to-date
%Possible solutions:
%A server-side library that keeps views up-to-date given some application-provided function
%Some SQL-like language to specify views/views updates
%Invariants?
%Tough with weak consistency and partial replicated data
%Dynamic partitioning?
%I suppose, keep track of objects that are accessed by other servers and re-partition them if it is accessed often? But this does not work well with buckets partitioning
%Support for new replicas/replicas removal?
%Already partially supported, would have to revisit this.


%Will likely need a picture of the proposed system design. Maybe also an example with data distribution + views? Maybe showing how an update to a data item not locally replicated generates a view update and said view update is applied in the other replica.

%``The Research Statement should explain the intended research work, including a description of the kind of issues and problem(s) to address, their motivation, the vision and ideas on how they may be tackled, their novelty, and the expected results. This presentation should be cohesively cross-referenced to the survey included in the previous chapter.''


%Albert did a introduction where he explains some challenges/difficulties of scaling applications and some choices
%Albert mentions a lot "we envision". Basically how they see applications in the future.
%Albert explains his initial work on Legion, shows some results and its works contributions towards the main goal
%Albert mentions work already done, tests done, applications he supports/intends to support and some limitations. On the next section, he means future directions of his work as a follow up to the limitations.
%When mentioning future works, he mentions some ideas on how to do the tasks, but generic ("We will develop a suit of protocols...")

%So I have to think of contributions, our goals, and explain what we have already done, what we plan to do, and compare with state of the art?


%Work I already did... the replication protocol... 2 phase-commit inside each replica... the partitioning... the TPC-H implementation & testing... having PotionDB emulate other server configurations...

%Causality among objects (causal+)
%Parallel Snapshot isolation (PSI)
%Reads in the past. New technique to save on space stored (log effects and operations; rebuild state on demand)
	%Still need to improve: garbage collection
	%Garbage collection can be done without coordination with other replicas
%Partial replication
	%Access to objects not replicated locally
	%Efficient support for partial replication
	%Support for non-uniform replication (selecting which operations to replicate, also for downstream generating new operations)
%Rich objects/interfaces
	%CRDTs
%Views
	%Novel outside of strong consistency? Hmm, not really (Noria)
	%Reflects state that may not be replicated locally! (novel!)
	%Consistency challenge - must stay consistent with relevant objects (causal consistency)
	%Challenge of making the view complete without all the information in one place. How to address it replication-wise. How to have the query be efficiently maintenaned and instantly queried
	%New CRDTs for this (rich interface/objects)
%Transactions
	%Providing read-write transactions, with some guarantees too (isolation inside the replica e.g.)
%Partitioning?
%Extended some existing CRDTs and defined new CRDTs
	%Max-min; extended TopSum with subtractions; added support for extra data on TopK, etc.
%Cross-replica transactions

%Stuff to do later?
%Garbage collection - figuring out versions that will no longer be needed
	%If said detection fails, can re-issue a transaction
%Consistency levels?
	%Already support snapshot isolation & monotonic reads (not really read your writes at the moment, but I should improve this). It's also read commited.
		%Data is still updated according to snapshot isolation, just that reads do not see it that way when working on fast mode.
	%Strong consistency?
%Automatic views/view updates?
	%As of now, the client keeps the views up-to-date
	%Possible solutions:
		%A server-side library that keeps views up-to-date given some application-provided function
		%Some SQL-like language to specify views/views updates
%Invariants?
	%Tough with weak consistency and partial replicated data
%Dynamic partitioning?
	%I suppose, keep track of objects that are accessed by other servers and re-partition them if it is accessed often? But this does not work well with buckets partitioning
%Support for new replicas/replicas removal?
	%Already partially supported, would have to revisit this.

%Mention all the work done so far in terms of testing... TPC-H implementation. Benchmark client too. Many different types of CRDTs implemented. Support to emulate other solutions.

%Main goal of this work is to design a novel replication model that leverages on partial and non-uniform replication
	%"We expect our model to reduce storage costs, reduce network bandwidth usage and potentially increase system throughput in geo-distributed or cloud scenarios"
