%!TEX root = ../template.tex
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% chapter2.tex
%% NOVA thesis document file
%%
%% Chapter with the template manual
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\typeout{NT FILE chapter2.tex}%

\chapter{Research Context}
\label{cha:research_context}

\todo{Is an Introduction like this okay? Or is just mentioning what will be discussed in this chapter enough?}

In this research work we propose to provide a geo-replicated, weakly consistent database which provides reliable access to complex analytical queries in near instant time.
To achieve this goal, multiple concepts need to be combined and leveraged on, e.g., view maintenance, replication, consistency.
For each concept, multiple different solutions exist and thus it is necessary to make informed choices and leverage on existing solutions/concepts.

This chapter organizes as follows.
Section \ref{sec:cap} describes the CAP theorem, presenting the foundations to the diversity of solutions for data storing.
%Section \ref{sec:consistency} discusses how data consistency is maintained in storage systems.
Section \ref{sec:consistency} discusses how storage systems handle data consistency.
Section \ref{sec:replication} introduces multiple concepts related with data replication, raging from replication synchronization to data distribution. 
Section \ref{sec:data_access} focuses on databases and efficient query provisioning, closing up with a discussion on existing solutions which apply the diverse concepts introduced in this chapter.
 \todo{Is this okay? @Reference to Section \ref{sec:replication}}

%Nowadays the internet is full of services, raging from entertainment purposes to e-commerce, information, business and others.
%Some of these services have a global scale with millions of users, with servers distributed around the globe.
%These services must be fault tolerant, provide low latency, high throughput, present data consistently and scale as the service grows in number of users.

%The aforementioned goals can often enter in conflict, thus many systems focus only on some of those goals.
%In this work we plan to 


\todo{Note: I am not sure if I should had talked about Antidote/Cure in high detail or not. I talked a bit about Cure on the Existing Systems subsection, is that enough, or does Antidote/Cure deserve a section of its own?}

\todo{Do I need to make more of a connection between the topics and PotionDB? E.g. motivate what we will be doing, how our solution is different/better, etc.?}

\section{CAP theorem}
\label{sec:cap}

\todo{Should the CAP theorem be included under consistency section, or leave it as it is?}

In an ideal world, distributed systems would all provide strongly consistent data, be always available and keep operating even in the presence of failures and network partitions.
However, the CAP theorem \cite{cap} states that it is only possible to, at most, provide two out of the three following properties:
\begin{enumerate*}[label=(\roman*)]
	\item Strong \textbf{C}onsistency - all replicas of a system must provide the same view of the data at any given time;
	\item \textbf{A}vailability - the system is capable of providing a reply to every client's request at any point in time;
	\item \textbf{P}artition-tolerance - the system keeps operating and evolving state correctly even in the presence of message loss and network partitions.
\end{enumerate*}

Typically, since network partitions are common on systems distributed across multiple regions \cite{understandingEC}, the choice tends to be between providing Strong Consistency or Availability.
It is noteworthy that one does not need to choose ``all or nothing'' \cite{understandingEC}. 
Systems providing \textbf{C}onsistency and \textbf{P}artition-tolerance (CP), can still be available in the presence of some faults, but will eventually stop to evolve state if too many nodes fail or too many messages are lost \cite{cap}.
On the other hand, systems providing \textbf{A}vailability and \textbf{P}artition-tolerance can still provide some consistency guarantees, known as Weak Consistency. However, the replicas will diverge at times and conflicting states may be observed due to the concurrent execution of operations \cite{understandingEC}.

\section{Consistency}
\label{sec:consistency}

In the literature, a plethora of consistency models have been specified \cite{linearizability, si, spanner, understandingEC, cops, dynamo, cassandra}, each one providing different guarantees to the developers in terms of the states that can be observed, as well as different requirements to deploy such models.
Despite the variety of models, they can be grouped as either being a form of \emph{Strong Consistency} or \emph{Weak Consistency}.

\subsection{Strong consistency}
\label{subsec:strong}

In strong consistency models, all clients observe the same state of the system independently of the replica contacted by the client(s).
%In order to achieve this, 
To achieve this, it is necessary to totally order all write operations executed in the system, which requires coordination among the different replicas \cite{linearizability,spanner,caerus,mdcc,detock,chainreaction}.
%Depending on the configuration of the system, even to execute a single read operation, many replicas may need to be contacted. %Citation?
Depending on the system configuration, even to execute a single read operation it may be needed to contact a majority of replicas or a master replica \cite{spanner,slog}.
This is necessary in order to ensure the client always reads the latest correct state.
%This is necessary as at any point it must be ensured that the client reads the latest state.

The strong requirements of coordination imply that strongly consistent systems, in order to tolerate network partitions, may have to come to an halt (unavailable) when enough replicas can not be contacted to execute an operation, in order to prevent state divergence or incorrect values being returned \cite{sconekv,cure,chainreaction,dynamo}.
Furthermore, the costs of synchronization can be prohibitively high for large scale systems, specially if replicated across the globe, as latency for operations quickly racks up \cite{caerus,mdcc,detock}.
Also, due to the total order requirement, it is often difficult to scale to a high number of clients and maintain high throughput under contention \cite{krikellas2010strongly,dynamo}.
On the other hand, strongly consistent systems tend to be easier to develop applications on than weaker consistency, as they are easier to reason about and do not expose consistency anomalies \cite{spanner,sconekv,slog,chainreaction,dynamo}.

A few models implementing strong consistency have been proposed in the literature \cite{linearizability,si}, such as: serializability, linearizability and snapshot isolation.
Recent research focuses on lowering the number of round trips needed for transaction execution, as well as leveraging on batching, data locality and other similar techniques, in order to reduce commit latency and improve throughput \cite{sconekv,caerus,detock,slog,tdsql}. %Should I add that another goal (by leveraging on data locality) is to reduce the number/percentage of geo-distributed/multi-DC transactions/commits? But in a way, that's reducing commit latency and improving throughput.
%Multiple models implementing strong consistency have been proposed in the literature \cite{linearizability, si}: serializability, linearizability and snapshot isolation to name a few.

\subsection{Weak consistency}
\label{subsec:weak}

Weak consistency models are considerably more relaxed in terms of requirements compared to strong consistency \cite{understandingEC}.
As such, they also provide weaker guarantees, making them harder to reason about and build applications on \cite{krikellas2010strongly,sconekv,slog,cops,cure,chainreaction,dynamo}.
Indeed, concurrent updates on the same objects may lead to undesired results or values.
%Replicas' state may diverge, with read anomalies and ``illogical'' (in the context of an application) states being observed \cite{krikellas2010strongly,detock,slog,cops,cure,dynamo}.
Replicas' state may diverge, with read anomalies and inconsistent states being observed \cite{krikellas2010strongly,detock,slog,cops,cure,dynamo}.
For example, in a group communication system, under some consistency models, user A may see user B replying to user C's message before he even sees user C's message.
This can be very confusing to the users.
However, the weaker requirements of weakly consistent systems gives potential to many advantages compared to strong consistency: better fault tolerance (operations keep executing even under network partitions), lower latency and higher scalability in both throughput and number of clients \cite{slog,cops,cure,dynamo}.

One of the most basic consistency models is eventual consistency \cite{understandingEC}.
In short, eventual consistency provides only one guarantee: when updates stop occurring, eventually the state of the replicas will converge.
However, several issues are left in the open - e.g., what happens when the same object is concurrently updated? Which states can be observed on reads? How do the states evolve as updates are being applied?
Thus, many other weak consistency models have been studied in the literature \cite{understandingEC, session, cops, dynamo, cassandra, walter, pnuts}, e.g. causal consistency, causal+, monotonic reads, read committed, per record sequential, PSI, etc.

Causal consistency \cite{cops,eiger,chainreaction,cure} is of particular interest as it provides useful guarantees.
In a general form, causal consistency ensures that operations casually related \cite{lamport2019time}, i.e. in which one happens before the other, are seen according to said order by every client.
This means that, e.g., a read that happens after a write must reflect the effects of said write.
With causal consistency, the aforementioned example of group communication system would no longer be possible - user A would always see user B's reply after seeing user C's message, as they are causally related \cite{lamport2019time, walter}.

\todo{Any system that provides causal consistency on the level of a single object only? I couldn't find any other than... a CRDT itself. (I do not think PNUTS counts as single-object causality, as some authors call it "same-key sequential consistency")}

Causality can be provided either in the context of a single object (i.e., causality between different objects is not ensured) or between multiple objects \cite{understandingEC}.
The former is tougher/costly to maintain, as an order needs to be maintained between potentially all objects, but more useful.
For example, in a online shopping system which allows users to track when a product is restocked, 
%it is desirable 
we want
for the notifications sent to the users to be causally related with the restocking action, as we do not want users to receive the notification before they can see the product in stock.
%For this use case, causality between objects is useful.
For this use case (as well as the group communication example, among others), causality between objects is very useful and eases application development.

Causal+ consistency \cite{cops,chainreaction,cure} extends causal consistency by ensuring replicas do not diverge forever, through techniques that solve concurrency conflicts deterministically in all replicas.
%Causal+ consistency \cite{cops,chainreaction,cure} still applies the same ideas of causal consistency, but also ensures the replicas do not diverge forever, by applying techniques to solve concurrency conflicts deterministically in all replicas.
By definition, causal consistency does not ensure replicas converge \cite{cops,chainreaction}.

Monotonic reads \cite{session, understandingEC} gives one simple to understand guarantee: consecutive reads on the same object must always return the same value or a more recent value.
This means that after a value is read, it is not possible to read again an older value.
It is however possible to read stale data, or keep reading the same value despite more recent values existing.
In distributed systems, it is important to define if this guarantee is given at a single server level or across servers, as the latter may imply having to wait for data to arrive and be processed if the client issues reads in different servers.

Snapshot Isolation (SI) is an interesting model.
%The intuition behind SI is that each transaction sees a consistent view of the database \cite{si}.
%To be more precise, it sees a ``snapshot'', where all the objects are at the same version and the database is in a consistent state.
The intuition behind SI is that each transaction sees a consistent view of the database \cite{si} - a ``snapshot'', where all objects are at the same version and the database is in a consistent state.
Under SI, the transactions in a replica are totally ordered, thus preventing concurrency conflicts when updating the same objects.
This implies coordination is required to commit a transaction.
In SI, reads on a transaction are executed on a snapshot, never blocking.
Writes are applied on a higher version when the transaction commits, if no other transaction that committed in the meantime modified the same object(s).
However, some anomalies can still occur under SI.
E.g., consider objects 'c' and 'd', and transactions 'A' and 'B'.
Consider now that both transactions read 'c' with value of 5. 
If transaction 'B' updates 'c' to 10, and transaction 'A' reads 'c' and copies the value of 'c' to 'd', both transactions can commit.
However, it is possible for 'B' to be ordered before 'A' and yet, end up with 'c' = 10 and 'd' = 5.
This would not be possible under serialization.

Parallel Snapshot Isolation (PSI) extends SI by allowing transactions issued in different replicas to commit in different orders \cite{walter}.
%Parallel Snapshot Isolation (PSI) is an extension of SI that allows transactions issued in different replicas to commit in different orders \cite{walter}.
PSI is useful for weak consistency systems, as they can provide strong guarantees when clients access only one server and, e.g., causality between updates on different replicas, thus still allowing high throughput and low latency in some cases.
However, under PSI, coordination may still be needed for some transactions, as PSI guarantees there are no write conflicts.
\todo{I do criticize this hybrid approach in the geo replication section.}

Sovran et. al. \cite{walter} define PSI+cset as an extension of PSI.
This extension states that if all operations of a given data type are commutative between themselves (e.g., a counter with only increment and decrement), then updates on objects of said data type never conflict.
%This extension notes that if an object's data type operations are all commutative between themselves (e.g., a counter with only increment and decrement), then updates on 
%This extension notes that if two concurent updates to the same object are done with operations that are commutative (e.g., an increment and decrement of a counter), then there is no write-conflict and thus both transactions can commit.

\todo{Is this paragraph ok? Or should I say something like "PotionDB will provide (...)"?}
In PotionDB we provide PSI+cset, with causal+consistency (hereafter causal consistency) for transactions executed in different servers.
We provide causality even if the client contacts a different server. \todo{Is "We provide causality even if the client contacts a different server" clear? My intention: when a client stops contacting its server and contacts a different one, causality is still kept, as he/she will ask for a snapshot more recent than the last one he/she observed.}
Our causality guarantees are cross object.
The views in our system are directly co-related to data of other objects, thus when a client observes a change in one of such objects, it must also observe the corresponding change in the views.
Causality between objects and views is very challenging in PotionDB's scenario, as data is partially replicated.
Thus, views can be related to objects of different partitions, without a single server having said view and all of its objects.
%Thus, it is possible for views to be related to objects of different partitions, without a single server having said view and all of its objects.
Causality must still be ensured despite this difficulty.
In PotionDB synchronization is never required to commit a transaction, as all our objects only support commutative operations - thus there are no write conflicts. \todo{Should this last phrase be in this subsection, or on the next one, after I talk about CRDTs?}

%In PotionDB we provide PSI, with causal+ consistency (hereafter causal consistency) for transactions executed in different servers. Our causality guarantees are cross object.
%The views provided in our system are directly co-related to the data of other objects, thus it is desirable that when a client observes a change in said objects, they can also observe the change in the views.
%We provide causality even if the client contacts a different server.
%Note that providing causal consistency is specially challenging in PotionDB's scenario, as data is partially replicated.
%Thus, it is possible for views to be related with objects of different partitions, and there not exist a single server which has both the view and all said objects.
%Causality must be ensured despite such difficulty, i.e., causality between objects that are not even replicated in the same server.

To accommodate use cases in which higher throughput is desirable and causality is not needed for the read, we also provide monotonic reads as long as the client is sticky to one server.
Note that even with monotonic reads, in PotionDB the client only observes states generated by committed transactions.
%We note that even in this scenario, the client always reads state left by some committed transaction, i.e., there is no interference from ongoing transactions. %i.e., no dirty reads, read commited.

\subsection{CRDTs and other conflict resolution techniques}
\label{subsec:crdt}

\todo{After talking about PSI in detail (namely that it does not allow write conflicts) and that with cset extension there is no conflicts, maybe this section is a bit confusing?}
\todo{A possible idea would be to say in the previous section that we provide PSI (and not mention @ commutativity), and then here detail that actually we provide the PSI+cset extension}
%CRDTs/operational transformation
Both with eventual and causal consistency, concurrent updates on the same objects can occur and lead to conflicts \cite{understandingEC}.
For instance, if an element is concurrently added and removed from a set, it is not clear what should be the final state.
Concurrency conflicts need to be dealt with, in order for all replicas' state to converge.
Solutions for handling conflicts can range from preventing them through coordination (as in PSI without csets), totally ordering operations following some criteria (e.g., using logical clocks and replica IDs), letting the user/application deal with the conflicting state (e.g., in a register with concurrent writes, use application logic to decide which value to stay) or forcing all operations to be commutative (if operations commute then the result will be the same, independently of the order by which they are applied).

In PotionDB we make usage of CRDTs \cite{crdt} to represent our datatypes.
CRDTs, or \textbf{C}onflict-Free  \textbf{R}eplicated \textbf{D}ata \textbf{T}ypes, are data types designed to be used in large-scale distributed systems offering weak consistency.
Concurrency conflicts are solved by the CRDT itself, by designing operations to be commutative.
Updates and reads can execute locally without synchronization, with all replicas of a CRDT eventually converging.
Updates get propagated asynchronously.
This allows for high availability and low latency on accessing CRDTs, as it is even possible to cache CRDTs on the client-side \cite{legion, swiftcloud, castineira2015collaborative}.
Multiple policies for handling conflicts are available for the same datatypes, allowing CRDTs to more easily adapt to the needs of each application \cite{crdtMultipolicy, crdt}.

Alternative solutions based on commutativity have been studied in the literature. 
One such solution is RADTs (\textbf{R}eplicated \textbf{A}bstract \textbf{D}ata \textbf{T}ypes) \cite{radt}, which are similar to CRDTs but with more limited conflict resolution policies.
Another solution is OT, \textbf{O}perational \textbf{T}ransformation \cite{ot, otCorrectness}.
The intuition behind OT is that conflicting operations can, when arriving on a replica, be ``transformed'' to be commutative and thus not conflict.
However, it is usually considered harder and more error-prone to design said transformations instead of, as in CRDTs, designing operations to be commutative from the start \cite{otCorrectness, crdt}.
We believe CRDTs' ease of use and flexibility with solving conflicts justify its usage over OT-based solutions in PotionDB.


\section{Replication}
\label{sec:replication}

%Must talk here about replication, partial replication, non-uniform replication and geo replication. May also have to talk here about op vs state based CRDTs and potentially state machine replication.

In distributed systems, replication is essential to ensure data is available in multiple sites.
Depending on the system, replicating data may lead to higher fault-tolerance and/or lower latency when accessing data, as well as higher scalability.
However, it also implies higher storage, network and computational costs.

It is thus important to tailor replication specifically to the intended use-case of the system.
Factors like consistency model, fault model of the system, geo-distribution of the servers, read/write ratio and types of operation affect the choice on how to replicate data efficiently.
As will be seen in this section, in PotionDB we combine ideas from multiple concepts of replication to provide a novel replication model that is well suited for PotionDB's use case.

\subsection{Synchronous and asynchronous replication}
\label{subsec:syncAsync}

\todo{Please check this section carefully... I don't feel very secure about what I say in this section. Suggestions are more than welcome.}

In synchronous replication, an update must be replicated to other replicas and confirmed by a subset (usually a majority) of those replicas before the update is considered completed.
This is usually applied by strongly consistent systems in order to keep the replicas up-to-date and avoid concurrency conflicts \cite{dynamo, spanner}.
These systems usually employ mechanisms such as Paxos \cite{paxos} or Total Order Broadcast \cite{tob} to totally order the operations, or a ``master-slave'' model in order to sort the operations in one place.
Combining these mechanisms with synchronous replication allows to ensure that no operations are lost and clients always read the latest, correct state \cite{spanner}.

\todo{Is it worth it to make a small section to present more information on state machine replication and paxos?}

In asynchronous replication, an update can be confirmed before it is sent to other replicas.
This makes it ideal for weakly consistent systems as it allows them to quickly confirm operations to the clients \cite{cops}.
Replication happens in the background, possibly even delayed in order to allow grouping with other operations for efficiency \cite{dynamo, cure}.

Synchronous replication provides strong guarantees about the state evolution of the systems, ensuring data and updates are not lost and always consistent.
However, having to wait for the confirmation of other replicas not only limits fault-tolerance \cite{spanner} but also raises considerably the latency of each operation when replicas are far-away, making it unusable for geo-distributed systems or scenarios where low latency is essential \cite{slog}.
\todo{After writing of geo-replication, can probably put here more references.}
Asynchronous replication can lead to lower latency and higher throughput executing operations, due to not having to wait for other replicas' replies.
It also has lighter network requirements.
However, state diversion and concurrency conflicts may happen due to the lack of coordination among replicas, which need to be dealt with \cite{dynamo}.
While asynchronous replication may lead to more fault-tolerant systems \cite{dynamo, cassandra, cops}, namely in the presence of network partitions, controversially it makes the fault-detection more difficult to detect, as replicas can take un-bounded amounts of time to reply \cite{cap}.

\todo{Either here or in Research Context I should mention we make usage of async replication and explain why (avoid latency increase from geo-replication; better fault tolerance; sync not needed for our use-case)}

\subsection{Operation and state based replication}
\label{subsec:operationStateReplication}

Replicas can be kept up-to-date in different forms.
One important decision is on how to propagate changes to the state.
In the literature, solutions for propagating changes are usually grouped as being \emph{state-based} or \emph{operation-based} \cite{crdt}.

\todo{Do I need to say that for state-based CRDTs, states should form a monotonic semi-lattice, and explain that? As in, should I go into detail?}
\todo{Also should this section be more CRDT-focused (as in, the case of CRDTs in particular) or generic (the generic concept of state-based/op-based replication)}
\todo{Finally, maybe this should be grouped with the CRDT section instead of replication section.}

\paragraph{State-based \cite{crdt}.} Updates are propagated by sending the entire object state to other replicas.
If updates can happen in multiple replicas, the coming state may have to be merged with the existing state, so that the effects of operations in the existing state are not lost.
This merges and state propagation must be done in such a way that it is guaranteed that, eventually, all replicas converge to the same state.
Extra care must be taken when applying consistency models stronger than eventual consistency.
E.g., in causal consistency, the merge must ensure causality is kept \cite{understandingEC}.

\paragraph{Operation-based \cite{crdt}.}
Updates are propagated by sending the operations directly (or their effects) to other replicas.
The receiving replicas must then apply the operations/effects on their own objects.
Care must be taken on how these operations are handled - depending on the consistency model, these operations may have to be delivered and applied according to some order, e.g., causal order or even total order.
It must also be ensured that every relevant operation is delivered everywhere, and conflicts from concurrency are handled properly to avoid states diverging forever.

\paragraph{CRDTs \cite{crdt}.}
CRDTs are classified based on their type of propagation: state-based CRDTs and operation-based CRDTs (op-based CRDT), respectively.

In state-based CRDTs, to ensure states converge eventually, it is necessary for the ordered states of an object to form a monotonic join semi-lattice, and the merge operation must compute a least upper bound \cite{crdt, stateCRDT}. The intuition behind these concepts is that, for any two states, a merged state can be calculated that is not older than either of the merging states, and reflects the effect of all operations applied on both states.
Since states can be merged in any order, eventual delivery of states to every replica is enough.
Note that it is not necessary for every single state to be delivered, but rather, enough recent states that ensure, when updates stop happening, that all replicas reach the most, and same, up-to-date state.

In op-based CRDTs, every update must be delivered to every replica.
Furthermore, if not all operations are commutative (usually it is only required for concurrent operations to be commutative), then casual delivery is required from the network.

\paragraph{Shortcomings and optimizations.}
Both approaches have shortcomings.
Operation propagation can overwhelm a system if updates happen too often (too many small messages), while propagating states can be too expensive with big states, leading to big messages for every small state changes.
Optimizations can be done for both cases.
For operations, multiple can be grouped in a single message and applied in a row.
For states, it is possible to calculate differences of states (delta) and send those differences instead of the whole state.
This solution has been studied in the literature \cite{deltaAlmeida, deltaVan}, with CRDTs based on deltas being known as delta-CRDTs. Delta-CRDTs present considerable reductions on the message size compared to state-based CRDTs, but also have complex specifications \cite{deltaAlmeida}. Also, keeping replicas correctly up-to-date is more challenging (namely in terms of causal consistency \cite{deltaAlmeida}).

\subsection{Partial replication}
\label{subsec:partial}

Full replication is very popular in distributed storage systems \cite{sipre}. 
It allows for potentially high fault tolerance, as data is stored everywhere so many servers can be unavailable and, depending on the consistency model, still have the data reachable or, at least, safely stored.
Full replication also simplifies data access, as whenever updates or reads are issued, all data being accessed is present in one place.
However, it also comes with downsides: as the number of servers increases, more storage is needed, as all servers store all data.
Network and processing power costs also increase, as all servers either process updates to all objects or changes to their states \cite{sipre}.

As the number of servers and/or data-centers increases, replication costs may become prohibitively high.
In the literature two different mechanisms have been proposed to alleviate the replication costs:

\paragraph{Internal partitioning.}
The idea behind internal partitioning is to split the dataset inside each server/data-center, allowing the system to scale with respectively, multi-core CPUs or multiple machines.
Internal partitioning in the context of a data-center helps reduce the higher replication costs associated with increasing the number of servers, however it also introduces some overhead in accessing data across partitions \cite{dynamo, cops, mdcc}.
Synchronization between machines may be needed to execute transactions, but this may be acceptable as latency inside a data-center is low \cite{cops}.
While this process already helps in reducing the increasing replication costs, all data-centers still replicate all data (full replication), which may still waste storage and network resources.
Internal partitioning is often referred to as ``sharding''.
We also include under internal partitioning systems \cite{dynamo} in which each node is responsible for a subset of the data and replicates some other subsets for fault tolerance.
Under this replication scenario, however, it is difficult to provide good consistency guarantees or features such as transactions.

\paragraph{External partitioning}.
With external partitioning, each data-center may replicate a different subset of data.
This allows to further reduce storage, network and processing costs as, respectively, less data is stored, less updates need to be sent and processed in each data-center \cite{spanner, sipre}.
The replication mechanism associated with external partitioning is called partial replication.

Partial replication is thus of interest for large-scale services, in order to keep replication costs bearable as the number of data-centers increases.
It is even more useful for systems distributed all around the world (geo-distributed) in which there is locality on the data \cite{sipre, slog}.
E.g., in an international e-commerce application, data regarding customers, sales and stocks in Europe is likely not relevant in Asia.
With partial replication, the referred data could be replicated only in European data-centers and possibly one other region for fault tolerance.
Besides the aforementioned benefits of partial replication, another advantage also comes from this: if synchronization is required (e.g., to ensure strong consistency), transactions regarding European data only need to coordinate with European data-centers, instead of data-centers spread across the world \cite{spanner, sipre, slog}.
This can allow for lower latency and higher throughput.

On the other hand, external partitioning needs to be carefully done, taking in consideration application context.
As data-centers do not have all the data, it is of foremost importance to ensure that, for most transactions, the data accessed is all in a single data-center.
Otherwise, a transaction may require multiple data-centers to be contacted/synchronized with \cite{spanner, sipre, chronocache, slog}, even under weak consistency models such as causal consistency.
This may severely hinder throughput and lead to high latency, even under weak consistency models, which negatively affects the user experience \cite{eiger, mdcc}.
Thus, careful fine-tuning of the partition scheme is important, and studies have been done for systems to self-adjust to changes on access patterns, by automatically migrating data between data-centers \cite{slog}.
If there is no locality on data or some other criteria to externally partition data, full replication may be more appropriate, specially under weak consistency.

\todo{Do I need some concluding note here/reference to PotionDB? Is this too much information?}

%Need citation for the disadvantages/shortcoming of partial replication
%Need citation for the advantages
%Maybe list all (or some) papers that make use of partial replication

\subsection{Geo replication}
\label{subsec:geo}

\todo{This introduction may not be needed depending on what I say on... the Introduction itself. Also when I did this introduction, geo replication's section was before partial replication... does this intro still make sense?}

Many Internet services nowadays are deployed on a global scale with millions of users from all over the globe.
This leads to large-scale services having data-centers spread across the globe (geo-distributed), in order to ensure low latency to all users, as well as provide solid fault tolerance.
However, deploying geo-distributed systems is challenging due to the high latency between faraway data-centers \cite{mdcc, eiger, chronocache, slog}. Availability is also difficult if strong consistency is desired, as previously discussed in Section \ref{sec:cap}.
Geo replication concerns all systems which have a need to have their data replicated in multiple parts of the globe, usually with some parts being faraway from each other and, thus, have to employ mechanisms to deal with high latency.

A related concept is wide-replication.
Wide-replication shares similar concepts with geo-replication, namely the existence of multiple data-centers in different places. 
However the distance is usually somewhat limited (e.g., all data-centers located in the United States).
Some systems in the literature have been designed with wide-replication in mind \cite{spanner, cops}.
In this scenario however, having to coordinate with more than one data-center may be acceptable, as the latency is still low enough \cite{spanner} (below 100ms\footnote{For instance: Amazon's AWS servers in the United States have below 70ms latency among themselves, while the European's servers have below 50ms.}).
In geo-replication, this may become prohibitively expensive, as latencies between faraway data-centers are in the hundreds of milliseconds, which can easily degrade the user experience \cite{mdcc, eiger, chronocache}.

\todo{Does that part of "below 100ms" need any citation? And the amazon examples in the footnotes, does that need citation? I took from here: https://www.cloudping.co/grid}

Given the reasons above, solutions designed with geo-distribution in mind are required.
Solutions offering strong consistency for geo-distributed scenario exist \cite{mdcc, slog}, but they usually have very high latency, reduced availability and possibly low throughput.
Some systems try to go around those shortcomings \cite{mdcc, chronocache, slog, walter}, e.g. by making use of locality with partial replication, but they still suffer from the same problems when data from multiple data-centers needs to be accessed.
Also, they tend to even have more limitations on availability of some data, as the partial replicas tend to be nearby.
Other systems offer strong consistency for data access in one data-center and weaker consistency for data access across data-centers \cite{cops, eiger, walter}.
While the trade-off is interesting, it still leads to applications having to deal with issues inherit to weak consistency. E.g., updates done in different data-centers will be replicated under weak consistency, thus clients still observe the effects of weak consistency.

Finally, solutions offering weak consistency are attractive for geo-replication, as they coupe more easily with high latency, partial replication and are more scalable with the number of users \cite{eiger, cure, walter}.
However, some systems offer only eventual consistency (e.g. Dynamo), which is difficult for application programmers to work with due to its very weak guarantees.
Thus, research has been done in trying to bring causal consistency to wide-distributed and geo-distributed systems \cite{cops, eiger, saturn, cure, walter}, with causality across objects, specially under partial replication, being challenging.
The weak consistency systems can provide very low latency due to usually not needing to coordinate with other servers, but even with causality, it can be difficult to program applications on top of such systems. %Should cite here the one system that tries to provide a relational view on geo distributed.

\todo{Not sure if the text below is necessary/should be in some other form. Likely need to revisit this after other chapters are written/developed}

PotionDB is a key-value store which is geo-replicated and allows to provide both partial replication and full replication.
System administrators can decide, for each group of objects, if they should be fully replicated or partially replicated, and, in case of the latter, in which servers each group should be replicated in.
PotionDB also makes usage of internal partitioning inside each server to make usage of multi-core CPUs.
The main challenge of PotionDB consists in providing consistent views that concern global data, under a form of weak consistency (causal consistency) and without all the required data being in a single server.
Provisioning of said views should only require interacting with a single server also, to ensure low latency.

\subsection{Non-uniform replication}
\label{subsec:nonuniform}

The usage of partial replication can already reduce networking and storage costs, by reducing the amount of places each object is replicated on.
However, it does not help with the case of popular objects that may have a big size or have a lot of updates.

\todo{A better example may be needed for this?}
Consider a worldwide e-commerce application, with hundreds of thousands of different products (like Ebay or Amazon).
A naive way to maintain a leaderboard of the ``top 10 most sold products'' would be to use an object which keeps a sum of how many units each product sold.
This would imply having to generate an update for this leaderboard whenever a product is sold somewhere, and have this update be propagated and processed everywhere.
However, the vast majority of the updates would not affect the visible state (the top 10) of the leaderboard - updates to an element not on top do not affect the result of reads of the said leaderboard.

Non-uniform replication \cite{nonUniform} is useful for situations as the one described above.
With non-uniform replication, only updates that may change the visible state of the objects would be propagated.
Other updates would only be stored locally (and potentially to a few other replicas for fault tolerance) for the case in which it may become relevant later on.
Using non-uniform replication, it is possible to save on both storage and network costs, as less operations are replicated, processed and stored.
In some objects, it is even possible to discard operations entirely \cite{nonUniform} - e.g., in an object responsible to keep the max number of sales in a day of a product, any number of sales of a day that is below the max does not need to be propagated and can be discarded.
In short, the idea behind non-uniform replication is to reduce the amount of data that needs to be propagated and stored, while still ensuring any query can be served fully at any replica.

Cabrita et. al. \cite{nonUniform, nonUniformThesis} propose some non-uniform CRDTs.
Namely, they propose Top-K, Top-Sum (similar to Top-K but supports increments to entries), histograms and a filtered set as CRDTs using non-uniform replication.
Another concept that is related is computational CRDTs \cite{computationalCrdt}.
The idea behind computational CRDTs is that, for some objects, the client is not interested in the actual state of the object but rather the result of some computation.
For example, in an object that keeps an average, the client is only interested in the average value - the values of each element that was added to the average is not relevant.
Thus, it is not necessary to keep each individual value, but rather enough information to calculate the result (in this case, the sum of all adds and the number of additions).

Both computational and non-uniform CRDTs are useful for PotionDB.
We can leverage on both concepts in order to supply many different kinds of objects, which allow us to provide very rich views or summaries of data. 
This, in turn, allows us to efficiently and easily answer many different kinds of queries, keeping the storage and network overheads low.
We have already developed new CRDTs based on these concepts, namely a max/min CRDT and an average CRDT.
We have also extended the design of TopSum CRDT to support decrements.
With the CRDTs already implement, many SQL-like function queries are already supported, such as \emph{order-by}, \emph{group by}, \emph{limit}, \emph{average}, \emph{sum}, etc.
%We also extended both TopK and TopSum to support extra ``data'' to each entry

\todo{Not sure if the text above should be here or in the research statement. Also should the research statement include an example of how we convert a query to objects in PotionDB?}

%Give examples of objects.
%Talk about computational CRDTs
%Explain how this helps PotionDB


%Idea: reduce storage and network costs (not every data needs to be replicated everywhere). However, still replicate enough data to reply to any query

%Mention about usefulness. Mention about some datatypes, both proposed in the paper and the ones I made. Mention about computacional CRDTs also having similar ideas (I'll need to check a paper on this). In the uniform paper they mention the similarities with computacional CRDTs (and how non-uniform is an extension of it).

%Will have to explain about the differences in states - same observable state vs same state.

%Mention how this can be combined with partial replication. And how it all fits in PotionDB.

%I might have to mention about non-uniform eventual consistency... but I'd rather not.


%General introduction about why replication is needed, its usage/beneficts and also costs
%Then go into each category of replication. Async vs sync? Then geo? Then partial? Then op vs state CRDT and end with non-uniform?

\section{Data access}
\label{sec:data_access}

Most online services nowadays require storage solutions in order to support said services.
Often, databases are leveraged on in order to store and provide an interface to access the service's data.
E.g., in an e-commerce application, databases may be used to store the stocks of products, shopping carts, customer information, etc.
It is expected for databases to provide an easy and quick access for data, while also managing how the data is stored and kept consistent.

While many solutions for presenting data exist, two of the most common ones are key-value stores and relational databases.

\paragraph{Key-value stores.}
Most key-value stores present a very simple interface with two basic operations - \emph{get} and \emph{put} \cite{dynamo, cops, cure, walter, pequod}.	%key-value stores
The former allows users to obtain the state of an object, while the latter updates or stores an object.
Each object is accessed by providing a key that uniquely identifies it.
Some databases only provide opaque objects, while others may provide more rich interfaces by supporting objects such as counters, maps and sets, alongside appropriate operations on those, or other features such as transactions \cite{cure, walter, pequod}.	%DBs providing extra objects/ops
PotionDB provides a key-value interface, with support for many different kinds of objects, operations, transactions and even for materialized views.

\paragraph{Relational databases.}
Relational databases provide a so-called relational model \cite{spanner, eiger, noria}. %DBs with relational model
In relational databases, data is organized in tables, with possible relationships between tables and the columns of different tables.
E.g., each entry in a table ``customers'' may refer to one or more entries in the table ``addresses''.
Usually, columns represent the fields of a table, while each row represents one entry (data) in the table.
Many systems provide SQL (\textbf{S}tructured \textbf{Q}uery \textbf{L}anguage) or a SQL-like language \cite{sequel, spanner, noria, mongoVScassandra} to define and access the data, which allows for a rich interface in accessing data and easy processing of said data.
For example, with a SQL query, it is possible to obtain directly from the database the average of salaries of users in a given company.
In comparison, in key-value stores, usually one would have to obtain multiple objects, calculate the average and then present the result, or store the pre-calculated result under some key.

Key-value stores are easy to use due to their simple interface \cite{dynamo}. 
Usually they offer weak forms of consistency, stressing on performance and fault tolerance \cite{dynamo, cops, cure}.
On the other hand, relational databases have a richer interface, which eases application development \cite{spanner, noria, eiger}, but are tougher to design and optimize for, being mostly used by databases providing strong consistency.
Some key-value stores provide extra features in order to facilitate application development by, e.g., providing many object types, extra operations, transactions \cite{cure, walter}.

\subsection{Queries}
\label{subsec:queries}

%Key-value stores typically offer simple interfaces, often gets and puts of data blobs. %Need a citation for this. Check some paper that claims this is not rich enough.
%However, this makes application development more difficult, as relationships between objects and their correctness have to be maintained by the application logic.
%This makes the relational model often provided by strong consistency systems interesting, as it eases application development.

%One way to ease this is to provide a key-value store with a rich interface - e.g., by providing many kinds of different objects and different reads/update operations.
%This still maintains the ease of development and high performance of key-value stores, while also easening application development.
%To achieve this, it is important to consider which kind of operations are often used by applications.
Applications access data in different ways.
Some mostly access the data directly, e.g., fetch information about a product in an e-commerce system.
Others may require summaries, aggregations, sorting of data or even more complex processing.

\textbf{O}n\textbf{l}ine \textbf{A}nalytical \textbf{P}rocessing, OLAP, is a method which consists on ways to analyse and organize data by executing analytical queries \cite{dbtoaster, viewSelection, optIncMaintenance, effMaintenance}.
These kind of queries are often used in the context of marketing, financial report or business decisions.
For example, in an e-commerce system, it may be useful to know the most popular products at any given moment, in order to keep their stock available and e.g. adjust prices or advertisement.
This poses a challenge, as very large amounts of data need to be analysed (all sales), with updates often occurring (new sales, new products) and in short periods of time (information needs to be available fast to make a decision while it is still relevant).
Views, indexes, windowed data and caches are some of the most common methods to provide these kind of queries \cite{noria, dbtoaster, pequod, txcache, viewSelection, optIncMaintenance, effMaintenance}.
Operations such as table joins, sums/averages of data, sorts, limits, group by and others are used by this kind of queries. 

The TPC-H benchmark \cite{tpch, dbtoaster, partView, lazyMaintenance} provides a specific example for a database managing sales data which aims to provide useful information for business decisions.
The queries listed in said benchmark provide a good example of the kind of queries executed in analytical scenarios.
One key requirement of analytical queries nowadays is to provide useful information with queries of quick execution, while many updates are happening on the background.

\todo{Maybe the paragraph below, or parts of it, should be in the research statement}
PotionDB is a geo-distributed database which aims to provide low latency for recurring queries.
Analytic queries fit such criteria (e.g., checking the most sold products is a recurring query).
We aim at providing such queries with a very low latency, similar to a normal query, while keeping the costs on storage low (partial + uniform replication).
As of now, PotionDB already supports many different kinds of CRDTs, providing solutions to answer queries often executed in OLAP scenarios.
Support for operations such as the ones referred before are already provided, and it is possible to support more if needed by adding new CRDTs to represent different kinds of objects or operations.
 

%Maybe end with a concluding note that later on more research can be done to find more types of applications/operations we may want to provide support for, or that PotionDB is extensible (e.g. - add more CRDTs)

%I might have to look into "OLAP". Online Analytical Processing. So for stuff like data mining, report writting, business reporting for sales, marketing, management, etc.

%Maybe I can start by looking into the SQL language. Then some papers providing strong consistency/SQL-like languages. There's one paper too providing weak consistency with relational model. Then also those papers of analytic queries/ChronoCache.

%Can't find anything in the existing papers. Will have to search some other way. I did find at least a few systems supporting materialized views.

\subsection{Speeding up queries - views and indexes}
\label{subsec:views}

In order to provide applications with easier and quicker access to data, mechanisms have been studied to complement direct data access \cite{noria, dbtoaster, pequod, txcache, viewSelection, optIncMaintenance, effMaintenance, lazyMaintenance, genIndex, partIndex, dbproxy}. 
Some of those mechanisms include usage of key-value stores as caches, indexes, views, data streaming, etc.
\todo{I probably need something more here to finish the paragraph... suggestions?}

\paragraph{Indexes.}
An index is a record whose goal is to speed up how quickly data is found \cite{partIndex, genIndex, mongoVScassandra}. %Probably just need one or two citations that define an index/explain what it is for.
In the context of a relational database, an index can be done on a column of a table to provide a quick way to access data by some criteria other than the primary key.
E.g., an index on users' data may provide quick access by phone number.
It can also be used to help group data in groups or sort more efficiently.
In non-relational databases indexes can be useful too. For example, a map can store all the keys of users, and each key can then be used to directly access the data of a given user.
In short, indexes are an useful and commonly used way to speed up queries and data access.

\paragraph{Views.}
A view consists in a different way to present data.
In the context of databases, it is defined as being the result of some query \cite{txcache}. %Definition of a view.
Views can be used to filter data (e.g., only show users with age above a certain value), or to accommodate subqueries that may be used by other queries (e.g., a join of two tables in the case of a relational database).
As such, they are useful for application developer, as it provides data abstraction by letting users define relationships between tables.
These views are re-calculated every time they are used, thus they do not use storage space but require processing time when queried.
Materialized views, on the other hand, are used with the goal of speeding up queries \cite{noria, pequod, txcache, partView, viewSelection, incMaintenance, effMaintenance, lazyMaintenance}. %Definition and goals of materialized views, preferably also downsides.
Materialized views cache the results of a given query, thus they can provide much quicker access to data, avoiding having to re-execute certain queries all the time.
However, it comes with its downsides - extra disk storage is needed and maintaining the views consistent and automatically updated is challenging, thus few systems implement them.
View maintenance is a common research topic. \todo{I think this last sentence does not need a citation given what I already cited before, but I can list them all again}

\todo{Not sure if the paragraph below should be here... or, at least, in this kind of form.}
PotionDB proposes to provide support for materialized views with incremental view maintenance.
Materialized views have the potential to speed up queries by multiple orders of magnitude \cite{partView, effMaintenance, lazyMaintenance}.
Given PotionDB's geo-distributed, partial replicated nature, the main aim with materialized views is to provide quick and efficient access to data that, without a view, could imply a long and complex query execution across many servers.
Another goal is to provide such views without requiring all the data referred by a view to be replicated everywhere, thus saving in storage and replication processing costs.
The views themselves should, however, be replicated everywhere relevant to provide low latency access to them.
This is thus a quite challenging and novel problem - to the best of our knowledge, PotionDB is the first system to propose providing materialized views in a geo-distributed, partially replicated, weakly consistent scenario.
The challenge comes from multiple aspects.
PotionDB's materialized views should have a low storage footprint compared to the data they refer.
Also, views may refer to data that is spread across many servers, without a single server having all the data required to compute the entire view.
Furthermore, it is essential for a view to be able to provide a reply to a query without executing complex computations or any communication (e.g., fetching data) with other servers.
Finally, our views must also be kept up-to-date and provide causal consistency.
 \todo{Not sure if this part of ``without executing complex computations'' is clear.}

\subsection{Existing systems}
\label{subsec:existingSystems}

\todo{Suggestions for a better title? Also does this need some sort of introduction?}

%Spanner		% Strong consistency
%MDCC			% Strong consistency, geo-distributed. Tries to do 1 round trip
%SLOG			 % Strong consistency, geo-distributed. Tries to do low latency commits by making use of locality
%Dynamo			% Weak consistency
%COPS or Eiger	%Causal consistency. Maybe can describe only COPS, and mention on the authors later proposed Eiger with write transactions and a column-family model
%Cure			%Causal consistency, and where PotionDB takes some inspiration from
%Walter			%Strong consistency locally, weak consistency cross-datacenter
%Noria			% Data-flow, materialized views
%ChronoCache	%Geo-distributed cache
%TxCache	%Cache with materialized views and transactional consistency
%DBToaster	%Materialized views, OLAP queries, TPC-H
%DBProxy %Maybe, I didn't refer to this one at all in the rest of the document.

%Either on each one make a comment on their downsides, or try at the end to conclude on what can PotionDB do better/general limitations of each group of solutions

\paragraph{Spanner.} Spanner \cite{spanner} is a distributed, strongly consistent database proposed by Google.
It is designed for wide-area replication: the authors claim that data can be globally replicated, but they also claim most applications will spread datacenters in a single continent.
Data can be partially replicated, thus reducing replication costs.
Each data contains multiple versions, allowing for lock-free reads at a given version.
However, in case objects from different partitions are read in the same transaction, the system may need to wait until the clock advances enough.

Paxos and 2-phase commit are used to ensure strong consistency.
For each group of servers participating in Paxos, one server is chosen as leader, being the one responsible for coordinating the transactions.
If a transaction evolves data distributed across more than one Paxos group, then two-phase commit is used.

Spanner makes use of meaningful timestamps.
To be more precise, the timestamps are based on real clock time.
Spanner makes usage of Google's TrueTime API, which provides real time with an announced uncertainty.
To keep the uncertainty low, GPS and atomic clocks are used.
It is due to these real-time based clocks that Spanner provides lock-free reads in the past without locks or coordination.

The usage of 2-phase commit limits fault tolerance and latency.
Due to transactions across multiple datacenters, the datacenters cannot be too faraway as it would lead to high latency commiting transactions.
However, this implies some clients will be faraway from the datacenters and experience high latency, thus may not be appropriate for some geo-distributed services.
The system may also come under to an halt under a network partition, due to not enough datacenters being reachable to commit.

\paragraph{MDCC.} MDCC \cite{mdcc} is a geo-distributed strongly consistent system which aims to provide low latency by reducing the number of round trips for commits.
In geo-distributed scenarios, round trips between faraway datacenters have high latency.
Thus, MDCC focus on committing transactions with a single round trip, instead of using e.g. two round trips as in two-phase commit (2PC).
To achieve this, the authors rely on concepts such as operation commutativity and on combination of multiple Paxos variants, namely Multi-Paxos, Fast Paxos and Generalized Paxos.
Despite MDCC relying on having a master replica for each object, MDCC also provides single round trip commits even without evolving the master by using ``fast quorums'', which helps in case the master is in a faraway datacenter.

MDCC still has its shortcomings.
It needs at least one wide area round trip, while some weak consistency systems can execute operations locally.
Furthermore, under certain conflicts, a transaction executing under a single round trip may need to abort and trigger two extra round trips, thus having worse performance than 2PC.
Thus, MDCC is not appropriate in scenarios where write conflicts can happen frequently.

\paragraph{SLOG.} SLOG \cite{slog} is a geo-distributed strongly consistent system which aims to provide low latency and high throughput.
They propose to make usage of data locality to provide low latency transactions.
All data in the system has a master replica.
Writes must always be directed at the master replica, and so do linearizable reads (they do support snapshot reads in other replicas).
However, if all the data a client wants to access is mastered in the same datacenter, and the client is close to it (data locality), then SLOG provides low latency as it commits without coordinating with other datacenters.
However, clients faraway from the master need to pay high latency even if the client is close to some replica of the object.
Transactions spanning masters in more than one datacenter must also pay high latency.

Other noteworthy features include the provision of high throughput even under high contention on data access, as well as automatic data remastering without hindering throughput considerably.
However, these features, alongside with low latency transactions, come at a cost - transactions must be deterministic; after a client submits a transaction the client cannot abort the transaction; transactions must be fully known during the planning phase, i.e., it does not support dynamic transactions.

\paragraph{Dynamo.} Dynamo \cite{dynamo} is a weakly consistent, highly available key-value store.
It provides eventual consistency with a simple get/put interface.
Dynamo leverages on multiple features in order to provide low latency and a resilient service at a large scale.
Data is partitioned across the nodes with a zero-hop DHT, with each node knowing the location of every data item.
Dynamo is also capable of leveraging on the different capacities of each server by using virtual nodes - more powerful servers represent multiple virtual nodes.
Each object is replicated in N virtual nodes, being said N configurable. Reads/writes must happen in, respectively, R/W replicas to be confirmed, being both parameters also configurable.
Write conflicts due to concurrent updates are handled by the application using Dynamo - Dynamo detects those conflicts, keeping multiple versions of the object and giving them to the application.
The application then merges the versions with its own logic when executing a read.
Practical experience suggests that version divergence is a rare occurrence.

Dynamo is totally decentralized, applying a P2P structure.
Gossip is used to handle changes to the membership and failures, as well as building the zero-hop DHT.
Dynamo's structure allows it to provide low latency replies even at the 99.9\% percentile, while ensuring high availability and data durability.
However, providing only eventual consistency, having conflicts be merged by applications and no object abstractions increases the difficulty of developing applications that use Dynamo.

\paragraph{COPS.} COPS \cite{cops} is a weakly consistent key-value store for the wide-area.
COPS defines and provides causal+ consistency, with causal relationships possible even between objects in different servers (data can be partitioned inside a datacenter but not across datacenters).
Read transactions are also provided in one of COPS' versions (COPS-GT) in order to read multiple objects in the same version.

Operations are linearizable inside a datacenter, and causally consistent across datacenters.
Extra metadata is stored in the database for the COPS-GT version, with multiple efforts for garbage collection in both COPS and COPS-GT.
However, providing read transactions comes at a cost - in some scenarios, COPS-GT has lower throughput compared to COPS.
Also, COPS-GT's metadata size may grow unbound under network partitions, has some garbage collection mechanisms can only happen when all datacenters can communicate with each other.
The lack of write transactions, read+write transactions and more rich objects are some shortcomings the authors address in Eiger \cite{eiger}.
Neither COPS nor Eiger offer support for partial replication across datacenters.

\todo{Should I describe Cure in higher detail since PotionDB is based on Cure?}
\paragraph{Cure.} Cure \cite{cure} is a key-value store providing causal consistency with geo-replication.
Cure focus on being always available while still providing as many guarantees and ease of use as possible.
Namely, it provides highly available, causally consistent transactions (both read, write and mixed), with clients reading a consistent snapshot.
It also provides CRDTs representing many different data types and with different, automated, ways of solving conflicts, unlike many other datastores only offering registers.
The combination of all these features is novel.

Cure presents some optimizations to ensure non-blocking reads, as well as ensuring consecutive snapshots are increasingly more recent even under failures.
However, under some scenarios performance is considerably lower than other solutions.
Clients may have to wait when doing failover datacenters.
It also does not support partial replication or other useful utilities such as views.	\todo{To be honest, I don't really know what to put here as a disadvantage.}

\paragraph{Walter.} Walter \cite{walter} is a geo-replicated key-value store with interesting semantics.
Walter supports partial replication and makes usage of two interesting features: preferred sites, where writes of an object are more efficient; counting sets (csets), a set where all operations are commutative (similar to a set CRDT).
Walter ensures no write-write conflicts are possible, thus not needing conflict resolution techniques.
Transactions in a given site can be fast commited, without synchronization with other sites, if all objects updated either are csets or are preferred on said site.
Thus with usage of data locality and csets, some applications may have good performance with Walter.
However, any transaction updating an object in another preferred site falls back to 2PC, which is slow in geo-distributed scenarios and may block under partitions.
Thus, if locality is not perfect, Walter suffers from similar downsides to strongly consistent systems.
The authors also define Parallel Snapshot Isolation (PSI), by being an extension of SI where different sites can commit concurrently as long as no write conflicts occur.

\paragraph{Noria.} Noria \cite{noria} is a streaming data-flow system aimed for web applications which tolerate eventual consistency.
Noria focus on addressing common problems of both data-flow systems and databases, namely, windowed data for the former and materialized views for the latter.
The main contribution is their partially-stateful data-flow model.
This model allows to serve queries on all data, yet only maintain partial state on the operators.
New entries are automatically added to the partial state, and unused entries evicted.
To further save on memory usage, an extensive re-use of operators is applied.
Noria addresses on how to efficiently provide queries and updates on such a system model.

Noria provides partially materialized views, which are incrementally maintained.
The authors claim providing materialized views for every query is feasible with Noria. It is also claimed that application development is easier compared to a cache + database setup as there is no need for manual caching or ad-hoc tables to store intermediate results.

Noria provides impressive performance, however it has its shortcomings.
State eviction from partial state is random, which is sub-optimal.
It does not support parametrized range queries (e.g., day > 10) nor multi-column joins; failures lead to inefficient recomputation of the data-flow.
Only eventual consistency is provided unlike in many other highly available databases, making application development harder.
Replication is also not addressed nor is geo-distribution, both essential to many web services nowadays.
Sharding is supported but some cross-shard operations are inefficient.

\paragraph{ChronoCache.} ChronoCache \cite{chronocache} is a mid-tier (between client and database) caching system for geo-distributed scenarios.
ChronoCache does not store only static data - it aims to store query results automatically and predict upcoming queries, executing them before they are issued by the client.
Thus, by having ChronoCache located close to the clients, query predicting can lead to savings in long round-trips.

ChronoCache does not use offline training.
Instead, it analyses clients' patterns, grouping queries in dependency graphs and analysing correlations between queries.
It also analyses odds of some queries being followed by others, as well as detecting queries issued in loops.
In the latter, ChronoCache can predictively execute the whole loop after the first query of the loop is issued by the client.

ChronoCache can help speed up applications with constant access patterns.
However, applications with considerable update ratios or with less repetitive access patterns may not benefit from ChronoCache as much as, e.g., from a materialized view.
ChronoCache is also limited by the availability of the underlying database, thus network partitions may lead to a system halt.

\paragraph{TxCache.} TxCache \cite{txcache} is a caching system focusing on providing strong consistency guarantees.
Many caching solutions provide only eventual consistency.
In contrast, TxCache provides snapshot isolation and causality guarantees.
This is achieved by keeping track of the versions of each database item.
Reads on TxCache are likely to be stale, but the user can set a staleness limit.
TxCache always ensures a consistent, even if stale, snapshot view of the database, easing application development compared to other caching solutions (e.g., memcached) that do not even provide transactions.
TxCache requires considerable modifications on the database in order to notify TxCache of data invalidity, as well as serve reads on a given version. Its API is, however, easy to use for applications, as applications only need to designate functions as "cacheable".
Internally, TxCache keeps a cache of query results with a validity range for each result - this allows to know in which snapshots can each result be used on for future queries.

TxCache has some considerable limitations.
Read-write transactions execute directly on the database to avoid anomalies, thus take no benefit from caching.
Only functions with no side effects, deterministic and not dependent on external factors (e.g., system time) can be cacheable (TxCache does not check this requirement, leading to possible development bugs).
Required changes to the database may be difficult/lengthy, leading to potential bugs.
Index or sequential scan queries lead to cacheable data that will be invalidated with any update to relevant tables, despite possible conditions on the queries allowing for smarter eviction strategies.

\paragraph{DBToaster.} DBToaster \cite{dbtoaster} is a database focused on managing large datasets with a constant update stream.
Efficient support of analytical queries is desired, thus DBToaster focuses on long-running queries concerning large sets of data.
DBToaster provides materialized views with incremental view maintenance, making usage of delta queries to efficiently keep the views up-to-date.
Techniques are applied to automatically generate materialized views, potentially dividing a materialized view in many parts for efficiency.
By making usage of delta-queries and view splitting, they define triggers which update most views in constant time for each update, avoiding costly loops and table joins.
Query re-writing rules are also provided, alongside an heuristic optimizer that uses said rules to further optimize views and their maintenance.

DBToaster shows that it is possible to answer complex queries with materialized views and have affordable maintenance costs.
However, some queries with nested aggregates still have to use full view rebuilding, as well as efficient support of aggregates other than sums.
Queries with inequality joins will still need joins to update the view.

\todo{Should I do some connection between DBToaster and PotionDB? In PotionDB we do incremental view maintenance too, but ad-hoc.}

%ChronoCache	%Geo-distributed cache
%TxCache	%Cache with materialized views and transactional consistency
%DBToaster	%Materialized views, OLAP queries, TPC-H
%DBProxy %Maybe, I didn't refer to this one at all in the rest of the document.


%\section{Thoughts}
%
%So here it is basically a very extended state of the art (similar to how I did in my master's? A bit smaller though)
%Which topics do I want to touch about here?
%\begin{itemize}
%	\item CAP
%	\item Consistency levels (probably a lot to cover here - from weak to strong, multiple types of weak, causality between objects/same object, etc.)
%	\item CRDTs
%	\item Antidote.
%	\item Partial replication
%	\item Non-Uniform replication
%	\item Geo replication
%	\item State partition???
%	\item Views/indexes. Namely materialized views.
%	\item Maybe typical database queries/operations (limit, sort by, etc.)? Has some relevance as we aim to provide views with CRDTs.
%	\item Other similar solutions (ChronoCache, etc. Check the paper). Must definitely mention things like Noria and predictive treaties.
%\end{itemize}
%
%What about subsections?
%\begin{itemize}
%	\item Replication
%	\item Consistency (can probably include CAP and state partition?)
%	\item Other solutions?
%\end{itemize}
%
%For consistency, I can check Albert's PHD plan for references. Same for conflict resolution techniques in CRDTs.
%Check our paper and my master's thesis for more topics/references too.
%
%Note: On research statement, I can talk about TPC-H, and even give an example on how to transform a TPC-H query into CRDTs.
%%\chapter{NOVAthesis Template \emph{User's Manual}}
%%\label{cha:users_manual}
%
%This phrase from the grant documents can be useful:
%''In this work we propose to design a novel replication model levering on the advantages of both partial and non-uniform replication. We will also conduct research on integrating multiple consistency models in our proposal, which is challenging due to transactions spanning multiple partitions and materialized views referring to multiple partitions. We will also implement our model in a database prototype.
%
%To talk about how difficult/novel the problem is, mention that: - we combine partial + uniform replication (novel) in a geo-distributed scenario + has to handle consistency for transactions spanning multiple partitions, ensuring both partially replicated objects and materialized views stay updated and correct.
%
%Check the FCT grant docs for objectives/contributions too (dynamic partition (add/remove replicas on the fly), stronger consistency models are particularly interesting to mention.)
%
%May be worth mentioning my Master's work (and the published paper) - if needed, I have the knowledge and experience to design new CRDTs, and also understanding of consistency levels.
