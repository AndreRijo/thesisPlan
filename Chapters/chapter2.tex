%!TEX root = ../template.tex
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% chapter2.tex
%% NOVA thesis document file
%%
%% Chapter with the template manual
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\typeout{NT FILE chapter2.tex}%

\chapter{Research Context}

TODO: A proper introduction to this

I think the CAP theorem can be included under consistency subsection, with a proper introduction/context. Likely will need to be shortened though.

I think I'll need to make more of a connection between these topics and PotionDB/choices in PotionDB. Checking our paper is a good way to see how that can be done.
Or maybe just leave that for the next chapter.

\section{CAP theorem}
\label{sec:cap}

In an ideal world, distributed systems would all provide strongly consistent data, be always available and keep operating even in the presence of failures and network partitions.
However, the CAP theorem \cite{cap} states that it is only possible to, at most, provide two out of the three following properties:
\begin{enumerate*}[label=(\roman*)]
	\item Strong \textbf{C}onsistency - all replicas of a system must provide the same view of the data at any given time;
	\item \textbf{A}vailability - the system is capable of providing a reply to every client's request at any point in time;
	\item \textbf{P}artition-tolerance - the system keeps operating and evolving state correctly even in the presence of message loss and network partitions.
\end{enumerate*}

Typically, since network partitions are common on systems distributed across multiple places \cite{understandingEC}, the choice tends to be between providing Strong Consistency or Availability.
It is noteworthy that one does not need to select ``all or nothing'' \cite{understandingEC}. 
Systems providing \textbf{C}onsistency and \textbf{P}artition-tolerance (CP), can still be available in the presence of some faults, but will eventually stop to evolve state if too many nodes fail or too many messages are lost \cite{cap}.
On the other hand, systems providing \textbf{A}vailability and \textbf{P}artition-tolerance can still provide some consistency guarantees, known as Weak Consistency. However, the replicas will diverge at times and conflicting states may be observed due to the concurrent execution of operations.

\section{Consistency}

%TODO: Put here a list of both strong and weak consistency models. "Just" use papers already used for other sections
%Cassandra may not really be needed. Dynamo may be useful for eventual; cops may be useful if I talk about causal+.

In the literature, a plethora of consistency models have been specified \cite{linearizability, si, spanner, understandingEC, cops, dynamo, cassandra}, each one providing different guarantees to the developers in terms of the states that can be observed, as well as different requirements to deploy such models.
Despite the variety of models, they can be grouped as either being a form of \emph{Strong Consistency} or \emph{Weak Consistency}.

\subsection{Strong consistency}

In models providing strong consistency, all clients observe the same state of the system independently of the replica contacted by the client(s).
In order to achieve this, it is necessary to totally order all write operations executed in the system, which requires coordination among the different replicas \cite{linearizability, spanner}.
Depending on the configuration of the system, even to execute a single read operation, many replicas may need to be contacted.
This is necessary as at any point it must be ensured that the client can read the latest state.

The strong requirements of coordination imply that strongly consistent systems, in order to tolerate network partitions, may have to come to an halt (unavailable) when enough replicas can not be contacted to execute an operation, in order to prevent state divergence or incorrect values being returned.
Strongly consistent systems tend to be easier to develop applications on than weaker consistency, as they are easier to reason about \cite{spanner}.
However, the costs of synchronization can be prohibitively high for large scale systems, specially if replicated across the globe, as latency for operations quickly racks up.
Due to the requirement of a total order, it also tends to be difficult to scale to many clients and maintain high throughput.

Multiple models implementing strong consistency have been proposed in the literature  \cite{linearizability, si}: serializability, linearizability and snapshot isolation to name a few.

\subsection{Weak consistency}

Models providing weak consistency are considerably more relaxed in terms of requirements compared to strong consistency \cite{understandingEC}.
As such, they also provide weaker guarantees, making them harder to reason about and build applications on.
For instance, concurrent updates on the same objects may lead to undesired results or values.
The states of the replicas may diverge, with read anomalies and ``illogical'' (in the context of an application) states being observed.
For example, in a group communication system, under some consistency models, user A may see user B replying to user C's message before he even sees user C's message.
This can be very confusing to the users.
However, the weaker requirements of weakly consistent systems gives potential to many advantages compared to strong consistency: better fault tolerance (operations can keep executing even under network partitions), lower latency and higher scalability in terms of throughput and number of clients.

One of the most basic consistency models is eventual consistency \cite{understandingEC}.
In short, eventual consistency provides only one guarantee: when updates stop occurring, eventually the state of the replicas will converge.
However, several issues are left in the open. For example: what happens when the same object is concurrently updated? Which states can be observed on reads? How do the states evolve as updates are being applied?

%TODO: Monotonic reads paper
Many other weak consistency models have been studied in the literature \cite{understandingEC, session, cops, dynamo, cassandra, walter}, e.g. causal consistency, causal+, monotonic reads, etc.
Causal consistency is of particular interest as it provides useful guarantees.
In a general form, causal consistency ensures that operations casually related  \cite{lamport2019time}, i.e. in which one happens before the other, are seen according to said order by every client.
This means that, e.g., a read that happens after a write must reflect the effects of said write.
With causal consistency, the aforementioned example of group communication system would no longer be possible - user C would always see user B's reply after seeing user A's message, as they are causally related \cite{lamport2019time, walter}.

\todo{Any system that provides causal consistency on the level of a single object only? I couldn't find any other than... a CRDT itself.}

Causality can be provided either in the context of a single object (i.e., causality between different objects is not ensured) or between multiple objects \cite{understandingEC}.
The former is tougher/costly to maintain, as an order needs to be maintained between all objects, but more useful.
For example, in a online shopping system which allows users to keep track of when a product is restocked, it is desirable for the notifications sent to the users to be causally related with the restocking action (as we do not want users to receive the notification before they can see the product in stock).
For this use case, causality between objects is desirable.

Causal+ consistency \cite{cops} still applies the same ideas of causal consistency, but also ensures the replicas do not diverge forever, by applying techniques to solve concurrency conflicts deterministically in all replicas.
By definition, causal consistency does not ensure replicas converge \cite{cops}.

Monotonic reads \cite{session, understandingEC} gives one simple to understand guarantee: consecutive reads on the same object, must always return the same value or a more recent value.
This means that after a value is read, it is not possible to read again an older value.
It is however possible to read stale data, or keep reading the same value despite more recent values existing.
In distributed systems, it is important to define if this guarantee is given at a single server level or across servers, as the latter may imply having to wait for data to arrive and be processed if the client issues reads in different servers.

\todo{This text will most likely need to be changed depending on how the introduction is written.}
In PotionDB we provide Causal+ consistency (hereafter causal consistency), including between different objects.
The views provided in our system are directly co-related to the data of other objects, thus it is desirable that when a client observes a change in said objects, they can also observe the change in the views.
We provide causality even if the client contacts a different server.
\todo{Likely need a better way to talk about this challenge.}
Note that providing causal consistency is specially challenging in PotionDB's scenario, as data is partially replicated.
Thus, it is possible for views to be related with objects of different partitions, and there not exist a single server which has both the view and all said objects.
Causality must be ensured despite such difficulty, i.e., causality between objects that are not even replicated in the same server.

To accommodate use cases in which higher throughput is desirable and causality is not needed for the read, we also provide monotonic reads as long as the client is sticky to one server.

\subsection{CRDTs and other conflict resolution techniques}

%CRDTs/operational transformation
Both with eventual and causal consistency, concurrent updates on the same objects can occur and lead to conflicts \cite{understandingEC}.
For instance, if an element is concurrently added and removed from a set, it is not clear what should be the final state.
Concurrency conflicts need to be dealt with, in order for all replicas' state to converge.
Solutions for handling conflicts can range from totally ordering operations following some criteria (e.g., using logical clocks and replica IDs), letting the user/application deal with the conflicting state (e.g., in a register with concurrent writes, use application logic to decide which value to stay) or forcing all operations to be commutative (if operations commute then the result will be the same, independently of the order by which they are applied).

In PotionDB we make usage of CRDTs \cite{crdt} to represent our datatypes.
CRDTs, or \textbf{C}onflict-Free  \textbf{R}eplicated \textbf{D}ata \textbf{Types}, are data types designed to be used in large-scaled distributed systems offering weak consistency.
Concurrency conflicts are solved by the CRDT itself, by designing operations to be commutative.
Updates and reads can execute locally without synchronization, with all replicas of a CRDT eventually converging.
Updates get propagated asynchronously.
This allows for high availability and low latency on accessing CRDTs, as it is even possible to cache CRDTs on the client-side \cite{legion, swiftcloud, castineira2015collaborative}.
Multiple policies for handling conflicts are available for the same datatypes, allowing CRDTs to more easily adapt to the needs of each application \cite{crdtMultipolicy, crdt}.

Alternative solutions based on commutativity have been studied in the literature. 
One such solution is OT, \textbf{O}perational \textbf{T}ransformation \cite{ot, otCorrectness}.
The intuition behind OT is that conflicting operations can, when arriving on a replica, be ``transformed'' to be commutative and thus not conflict.
However, it is usually considered harder and more error-prone to design said transformations instead of, as in CRDTs, designing operations to be commutative from the start \cite{otCorrectness, crdt}.
We believe CRDTs' ease of use and flexibility with solving conflicts justify its usage over OT-based solutions in PotionDB.

\section{Antidote}

Does this actually need a section of its own? Or should I refer it alongside other solutions and give more detail to this one?

\section{Replication}

%Must talk here about replication, partial replication, non-uniform replication and geo replication. May also have to talk here about op vs state based CRDTs and potentially state machine replication.

In distributed systems, replication is essential to ensure data is available in multiple sites.
Depending on the system, replicating data may lead to higher fault-tolerance and/or lower latency when accessing data, as well as higher scalability.
However, it also implies higher storage, network and computational costs.

It is thus important to tailor replication specifically to the intended use-case of the system.
Factors like consistency model, fault model of the system, geo-distribution of the servers, read/write ratio and types of operation affect the choice on how to replicate data efficiently.
As will be seen in this section, in PotionDB we combine ideas from multiple concepts of replication to provide a novel replication model that is well suited for PotionDB's use case.

\subsection{Synchronous and asynchronous replication}

\todo{Please check this section carefully... I don't feel very secure about what I say in this section. Suggestions are more than welcome.}

In synchronous replication, an update must be replicated to other replicas and confirmed by a subset (usually a majority) of those replicas before the update is considered completed.
This is usually applied by strongly consistent systems in order to keep the replicas up-to-date and avoid concurrency conflicts \cite{dynamo, spanner}.
These systems usually employ mechanisms such as Paxos \cite{paxos} or Total Order Broadcast \cite{tob} to totally order the operations, or a ``master-slave'' model in order to sort the operations in one place.
Combining these mechanisms with synchronous replication allows to ensure that no operations are lost and clients always read the latest, correct state \cite{spanner}.

\todo{Is it worth it to make a small section to present more information on state machine replication and paxos?}

In asynchronous replication, an update can be confirmed before it is sent to other replicas.
This makes it ideal for weakly consistent systems as it allows them to quickly confirm operations to the clients \cite{cops}.
Replication happens in the background, possibly even delayed in order to allow grouping with other operations for efficiency \cite{dynamo}.

\todo{If I refer to Antidote/Cure, I can probably use that citation above.}

Synchronous replication provides strong guarantees about the state evolution of the systems, ensuring data and updates are not lost and always consistent.
However, having to wait for the confirmation of other replicas not only limits fault-tolerance \cite{spanner} but also raises considerably the latency of each operation when replicas are far-away, making it unusable for geo-distributed systems or scenarios where low latency is essential \cite{slog}.
\todo{After writing of geo-replication, can probably put here more references.}
Asynchronous replication can lead to lower latency and higher throughput executing operations, due to not having to wait for other replicas' replies.
It also has lighter network requirements.
However, state diversion and concurrency conflicts may happen due to the lack of coordination among replicas, which need to be dealt with \cite{dynamo}.
While asynchronous replication may lead to more fault-tolerant systems \cite{dynamo, cassandra, cops}, namely in the presence of network partitions, controversially it makes the fault-detection more difficult to detect, as replicas can take un-bounded amounts of time to reply \cite{cap}.

\todo{Either here or in Research Context I should mention we make usage of async replication and explain why (avoid latency increase from geo-replication; better fault tolerance; sync not needed for our use-case)}

\subsection{Operation and state based replication}

Replicas can be kept up-to-date in different forms.
One important decision is on how to propagate changes to the state.
In the literature, solutions for propagating changes are usually grouped as being \emph{state-based} or \emph{operation-based} \cite{crdt}.

\todo{Do I need to say that for state-based CRDTs, states should form a monotonic semi-lattice, and explain that? As in, should I go into detail?}
\todo{Also should this section be more CRDT-focused (as in, the case of CRDTs in particular) or generic (the generic concept of state-based/op-based replication)}
\todo{Finally, maybe this should be grouped with the CRDT section instead of replication section.}

\paragraph{State-based \cite{crdt}.} Updates are propagated by sending the entire object state to other replicas.
If updates can happen in multiple replicas, the coming state may have to be merged with the existing state, so that the effects of operations in the existing state are not lost.
This merges and state propagation must be done in such a way that it is guaranteed that, eventually, all replicas converge to the same state.
Extra care must be taken when applying consistency models stronger than eventual consistency.
E.g., in causal consistency, the merge must ensure causality is kept \cite{understandingEC}.

\paragraph{Operation-based \cite{crdt}.}
Updates are propagated by sending the operations directly (or their effects) to other replicas.
The receiving replicas must then apply the operations/effects on their own objects.
Care must be taken on how these operations are handled - depending on the consistency model, these operations may have to be delivered and applied according to some order, e.g., causal order or even total order.
It must also be ensured that every relevant operation is delivered everywhere, and conflicts from concurrency are handled properly to avoid states diverging forever.

\paragraph{CRDTs \cite{15}.}
CRDTs are classified based on their type of propagation: state-based CRDTs and operation-based CRDTs (op-based CRDT), respectively.

In state-based CRDTs, to ensure states converge eventually, it is necessary for the ordered states of an object to form a monotonic join semi-lattice, and the merge operation must compute a least upper bound \cite{crdt, stateCRDT}. The intuition behind these concepts is that, for any two states, a merged state can be calculated that is not older than either of the merging states, and reflects the effect of all operations applied on both states.
Since states can be merged in any order, eventual delivery of states to every replica is enough.
Note that it is not necessary for every single state to be delivered, but rather, enough recent states that ensure, when updates stop happening, that all replicas reach the most, and same, up-to-date state.

In op-based CRDTs, every update must be delivered to every replica.
Furthermore, if not all operations are commutative (usually it is only required for concurrent operations to be commutative), then casual delivery is required from the network.

\paragraph{Shortcomings and optimizations.}
Both approaches have shortcomings.
Operation propagation can overwhelm a system if updates happen too often (too many small messages), while propagating states can be too expensive with big states, leading to big messages for every small state changes.
Optimizations can be done for both cases.
For operations, multiple can be grouped in a single message and applied in a row.
For states, it is possible to calculate differences of states (delta) and send those differences instead of the whole state.
This solution has been studied in the literature \cite{deltaAlmeida, deltaVan}, with CRDTs based on deltas being known as delta-CRDTs. Delta-CRDTs present considerable reductions on the message size compared to state-based CRDTs, but also have complex specifications \cite{deltaAlmeida}. Also, keeping replicas correctly up-to-date is more challenging (namely in terms of causal consistency \cite{deltaAlmeida}).

\subsection{Partial replication}

Full replication is very popular in distributed storage systems \cite{sipre}. 
It allows for potentially high fault tolerance, as data is stored everywhere so many servers can be unavailable and, depending on the consistency model, still have the data reachable or, at least, safely stored.
Full replication also simplifies data access, as whenever updates or reads are issued, all data being accessed is present in one place.
However, it also comes with downsides: as the number of servers increases, more storage is needed, as all servers store all data.
Network and processing power costs also increase, as all servers either process updates to all objects or changes to their states \cite{sipre}.

As the number of servers and/or data-centers increases, replication costs may become prohibitively high.
In the literature two different mechanisms have been proposed to alleviate the replication costs:

\paragraph{Internal partitioning.}
The idea behind internal partitioning is to split the dataset inside each server/data-center, allowing the system to scale with respectively, multi-core CPUs or multiple machines.
Internal partitioning in the context of a data-center helps reduce the higher replication costs associated with increasing the number of servers, however it also introduces some overhead in accessing data across partitions \cite{dynamo, cops, mdcc}.
Synchronization between machines may be needed to execute transactions, but this may be acceptable as latency inside a data-center is low \cite{cops}.
While this process already helps in reducing the increasing replication costs, all data-centers still replicate all data (full replication), which may still waste storage and network resources.
Internal partitioning is often referred to as ``sharding''.
We also include under internal partitioning systems \cite{dynamo} in which each node is responsible for a subset of the data and replicates some other subsets for fault tolerance.
Under this replication scenario, however, it is difficult to provide good consistency guarantees or features such as transactions.

\paragraph{External partitioning}.
With external partitioning, each data-center may replicate a different subset of data.
This allows to further reduce storage, network and processing costs as, respectively, less data is stored, less updates need to be sent and processed in each data-center \cite{spanner, sipre}.
The replication mechanism associated with external partitioning is called partial replication.

Partial replication is thus of interest for large-scale services, in order to keep replication costs bearable as the number of data-centers increases.
It is even more useful for systems distributed all around the world (geo-distributed) in which there is locality on the data \cite{sipre, slog}.
E.g., in an international e-commerce application, data regarding customers, sales and stocks in Europe is likely not relevant in Asia.
With partial replication, the referred data could be replicated only in European data-centers and possibly one other region for fault tolerance.
Besides the aforementioned benefits of partial replication, another advantage also comes from this: if synchronization is required (e.g., to ensure strong consistency), transactions regarding European data only need to coordinate with European data-centers, instead of data-centers spread across the world \cite{spanner, sipre, slog}.
This can allow for lower latency and higher throughput.

On the other hand, external partitioning needs to be carefully done, taking in consideration application context.
As data-centers do not have all the data, it is of foremost importance to ensure that, for most transactions, the data accessed is all in a single data-center.
Otherwise, a transaction may require multiple data-centers to be contacted/synchronized with \cite{spanner, sipre, chronocache, slog}, even under weak consistency models such as causal consistency.
This may severely hinder throughput and lead to high latency, even under weak consistency models, which negatively affects the user experience \cite{eiger, mdcc}.
Thus, careful fine-tuning of the partition scheme is important, and studies have been done for systems to self-adjust to changes on access patterns, by automatically migrating data between data-centers \cite{slog}.
If there is no locality on data or some other criteria to externally partition data, full replication may be more appropriate, specially under weak consistency.

\todo{Do I need some concluding note here/reference to PotionDB? Is this too much information?}

%Need citation for the disadvantages/shortcoming of partial replication
%Need citation for the advantages
%Maybe list all (or some) papers that make use of partial replication

\subsection{Geo replication}

\todo{This introduction may not be needed depending on what I say on... the Introduction itself. Also when I did this introduction, geo replication's section was before partial replication... does this intro still make sense?}

Many Internet services nowadays are deployed on a global scale with millions of users from all over the globe.
This leads to large-scale services having data-centers spread across the globe (geo-distributed), in order to ensure low latency to all users, as well as provide solid fault tolerance.
However, deploying geo-distributed systems is challenging due to the high latency between faraway data-centers \cite{mdcc, eiger, chronocache, slog}. Availability is also difficult if strong consistency is desired, as previously discussed in Section \ref{sec:cap}.
Geo replication concerns all systems which have a need to have their data replicated in multiple parts of the globe, usually with some parts being faraway from each other and, thus, have to employ mechanisms to deal with high latency.

A related concept is wide-replication.
Wide-replication shares similar concepts with geo-replication, namely the existence of multiple data-centers in different places. 
However the distance is usually somewhat limited (e.g., all data-centers located in the United States).
Some systems in the literature have been designed with wide-replication in mind \cite{spanner, cops}.
In this scenario however, having to coordinate with more than one data-center may be acceptable, as the latency is still low enough \cite{spanner} (below 100ms\footnote{For instance: Amazon's AWS servers in the United States have below 70ms latency among themselves, while the European's servers have below 50ms.}).
In geo-replication, this may become prohibitively expensive, as latencies between faraway data-centers are in the hundreds of milliseconds, which can easily degrade the user experience \cite{mdcc, eiger, chronocache}.

\todo{Does that part of "below 100ms" need any citation? And the amazon examples in the footnotes, does that need citation? I took from here: https://www.cloudping.co/grid}

Given the reasons above, solutions designed with geo-distribution in mind are required.
Solutions offering strong consistency for geo-distributed scenario exist \cite{mdcc, slog}, but they usually have very high latency, reduced availability and possibly low throughput.
Some systems try to go around those shortcomings \cite{mdcc, chronocache, slog, walter}, e.g. by making use of locality with partial replication, but they still suffer from the same problems when data from multiple data-centers needs to be accessed.
Also, they tend to even have more limitations on availability of some data, as the partial replicas tend to be nearby.
Other systems offer strong consistency for data access in one data-center and weaker consistency for data access across data-centers \cite{cops, eiger, walter}.
While the trade-off is interesting, it still leads to applications having to deal with issues inherit to weak consistency. E.g., updates done in different data-centers will be replicated under weak consistency, thus clients still observe the effects of weak consistency.

Finally, solutions offering weak consistency are attractive for geo-replication, as they coupe more easily with high latency, partial replication and are more scalable with the number of users \cite{eiger, cure, walter}.
However, some systems offer only eventual consistency (e.g. Dynamo), which is difficult for application programmers to work with due to its very weak guarantees.
Thus, research has been done in trying to bring causal consistency to wide-distributed and geo-distributed systems \cite{cops, eiger, saturn, cure, walter}, with causality across objects, specially under partial replication, being challenging.
The weak consistency systems can provide very low latency due to usually not needing to coordinate with other servers, but even with causality, it can be difficult to program applications on top of such systems. %Should cite here the one system that tries to provide a relational view on geo distributed.

\todo{Not sure if the text below is necessary/should be in some other form. Likely need to revisit this after other chapters are written/developed}

PotionDB is a key-value store which is geo-replicated and allows to provide both partial replication and full replication.
System administrators can decide, for each group of objects, if they should be fully replicated or partially replicated, and, in case of the latter, in which servers each group should be replicated in.
PotionDB also makes usage of internal partitioning inside each server to make usage of multi-core CPUs.
The main challenge of PotionDB consists in providing consistent views that concern global data, under a form of weak consistency (causal consistency) and without all the required data being in a single server.
Provisioning of said views should only require interacting with a single server also, to ensure low latency.

\subsection{Non-uniform replication}

\subsection{Replication in PotionDB}
%Likely not needed and I should just mention as we go by how we use each kind of replication.

%General introduction about why replication is needed, its usage/beneficts and also costs
%Then go into each category of replication. Async vs sync? Then geo? Then partial? Then op vs state CRDT and end with non-uniform?

\section{Views}

Here talk about views in common databases. What is their use, their advantages and disadvantages. Also talk about common queries/operations in relational databases which we will have to mimic. On the Research Statement, I should mention how we are mimicking/plan to mimic them.
Mention and discuss existing solutions.

%Maybe say that in PotionDB we decided to use CRDTs and explain the advantages. Mention the other possible solution (operational transformation) and why that one was not choosen.



%Focus on causal consistency
%Discuss about causality in same object and different object
%Make some conclusions on PotionDB about consistency.


%Present weak consistency in general
%Explain eventual consistency
%Mention strong and weak points of weak consistency
%Present other models, namely casual consistency. Explain in-object and cross-object casual consistency
%Refer our choice in PotionDB, possibly in another section.

\section{Thoughts}

So here it is basically a very extended state of the art (similar to how I did in my master's? A bit smaller though)
Which topics do I want to touch about here?
\begin{itemize}
	\item CAP
	\item Consistency levels (probably a lot to cover here - from weak to strong, multiple types of weak, causality between objects/same object, etc.)
	\item CRDTs
	\item Antidote.
	\item Partial replication
	\item Non-Uniform replication
	\item Geo replication
	\item State partition???
	\item Views/indexes. Namely materialized views.
	\item Maybe typical database queries/operations (limit, sort by, etc.)? Has some relevance as we aim to provide views with CRDTs.
	\item Other similar solutions (ChronoCache, etc. Check the paper). Must definitely mention things like Noria and predictive treaties.
\end{itemize}

What about subsections?
\begin{itemize}
	\item Replication
	\item Consistency (can probably include CAP and state partition?)
	\item Other solutions?
\end{itemize}

For consistency, I can check Albert's PHD plan for references. Same for conflict resolution techniques in CRDTs.
Check our paper and my master's thesis for more topics/references too.

Note: On research statement, I can talk about TPC-H, and even give an example on how to transform a TPC-H query into CRDTs.
%\chapter{NOVAthesis Template \emph{User's Manual}}
%\label{cha:users_manual}

This phrase from the grant documents can be useful:
''In this work we propose to design a novel replication model levering on the advantages of both partial and non-uniform replication. We will also conduct research on integrating multiple consistency models in our proposal, which is challenging due to transactions spanning multiple partitions and materialized views referring to multiple partitions. We will also implement our model in a database prototype.´´

To talk about how difficult/novel the problem is, mention that: - we combine partial + uniform replication (novel) in a geo-distributed scenario + has to handle consistency for transactions spanning multiple partitions, ensuring both partially replicated objects and materialized views stay updated and correct.

Check the FCT grant docs for objectives/contributions too (dynamic partition (add/remove replicas on the fly), stronger consistency models are particularly interesting to mention.)

May be worth mentioning my Master's work (and the published paper) - if needed, I have the knowledge and experience to design new CRDTs, and also understanding of consistency levels.
